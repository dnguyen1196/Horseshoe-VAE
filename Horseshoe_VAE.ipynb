{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Horseshoe-VAE.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "qOLmcpS_0MJE",
        "_xDkRxRa3mI4"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dnguyen1196/Horseshoe-VAE/blob/master/Horseshoe_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "-fsaZQ4qqUxL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "LiB6j9qcB4L7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Create VAE class\n",
        "\n",
        "NOTE:\n",
        "Compared to the reference VAE implementation, the main difference is in the calc_vi_loss function\n",
        "where the likelihood term has been augmented with the loss term to reconstruct the adjacency matrix\n"
      ]
    },
    {
      "metadata": {
        "id": "jHLhxy06GNQM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Install the necssary packages\n",
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IdhSSbsHzrsB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Standard VAE class"
      ]
    },
    {
      "metadata": {
        "id": "sYNQvpF8AQYP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            n_dims_code=16,\n",
        "            n_dims_data=32,\n",
        "            hidden_layer_sizes=[32],\n",
        "            encoder=True,):\n",
        "        \n",
        "        \"\"\"\n",
        "        q_sigma = 0.2\n",
        "        \"\"\"\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "\n",
        "        self.n_dims_data = n_dims_data\n",
        "        self.n_dims_code = n_dims_code\n",
        "        layer_sizes = (\n",
        "            [n_dims_data] + hidden_layer_sizes + [n_dims_code]\n",
        "        )\n",
        "        self.n_layers = len(layer_sizes) - 1\n",
        "\n",
        "        if not encoder:\n",
        "            layer_sizes = [a for a in reversed(layer_sizes)]\n",
        "\n",
        "        self.activations = list()\n",
        "        self.params = nn.ModuleList()\n",
        "        for (n_in, n_out) in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
        "            self.params.append(nn.Linear(n_in, n_out))\n",
        "            self.activations.append(F.relu)\n",
        "        self.activations[-1] = lambda a: a\n",
        "\n",
        "            \n",
        "    def forward(self, x):\n",
        "        # Note that if x contains multiple instance\n",
        "        # if x.shape = (num_sample, in_dim)\n",
        "        # then the output shape will be (num_sample, out_dim)\n",
        "        cur_arr = x\n",
        "        for ll in range(self.n_layers):\n",
        "            linear_func = self.params[ll]\n",
        "            a_func = self.activations[ll]\n",
        "            cur_arr = a_func(linear_func(cur_arr))\n",
        "        mu_NC = cur_arr\n",
        "        return mu_NC\n",
        "\n",
        "      \n",
        "class VariationalAutoencoder(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            q_sigma=0.2,\n",
        "            n_dims_code=16,\n",
        "            n_dims_data=64,\n",
        "            hidden_layer_sizes=[32],\n",
        "            hsp=False\n",
        "    ):\n",
        "\n",
        "        super(VariationalAutoencoder, self).__init__()\n",
        "        self.n_dims_data = n_dims_data\n",
        "        self.n_dims_code = n_dims_code\n",
        "        self.q_sigma = torch.Tensor([float(q_sigma)])\n",
        "        encoder_layer_sizes = (\n",
        "            [n_dims_data] + hidden_layer_sizes + [n_dims_code]\n",
        "        )\n",
        "        self.n_layers = len(encoder_layer_sizes) - 1\n",
        "        \n",
        "        if not hsp:\n",
        "            # Encoder network\n",
        "            self.encoder = NeuralNetwork(\n",
        "                n_dims_code=n_dims_code,\n",
        "                n_dims_data=n_dims_data,\n",
        "                hidden_layer_sizes=hidden_layer_sizes,\n",
        "                encoder=True,\n",
        "            )\n",
        "            # Decoder network\n",
        "            self.decoder = NeuralNetwork(\n",
        "                n_dims_code=n_dims_code,\n",
        "                n_dims_data=n_dims_data,\n",
        "                hidden_layer_sizes=hidden_layer_sizes,\n",
        "                encoder=False,\n",
        "            )\n",
        "        else:\n",
        "            # TODO: FactorizedInv-Gamma\n",
        "            # Use Ghosh's implementation of the factorizedInv-Gamma inference\n",
        "            # engine or reimplement his code with pytorch\n",
        "            \n",
        "            pass\n",
        "        \n",
        "\n",
        "    def forward(self, x_ND):\n",
        "        \"\"\"\n",
        "        Run entire probabilistic autoencoder on input (encode then decode)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        xproba_ND : 1D array, size of x_ND\n",
        "        \"\"\"\n",
        "        mu_NC = self.encode(x_ND)\n",
        "        z_NC = self.draw_sample_from_q(mu_NC)\n",
        "        return self.decode(z_NC), mu_NC\n",
        "\n",
        "    \n",
        "    def draw_sample_from_q(self, mu_NC):\n",
        "        ''' Draw sample from the probabilistic encoder q(z|mu(x), \\sigma)\n",
        "\n",
        "        We assume that \"q\" is Normal with:\n",
        "        * mean mu (argument of this function)\n",
        "        * stddev q_sigma (attribute of this class, use self.q_sigma)\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "        mu_NC : tensor-like, N x C\n",
        "            Mean of the encoding for each of the N images in minibatch.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        z_NC : tensor-like, N x C\n",
        "            Exactly one sample vector for each of the N images in minibatch.\n",
        "        '''\n",
        "        # Number of samples\n",
        "        N = mu_NC.shape[0]\n",
        "        \n",
        "        # The dimension of the code\n",
        "        C = self.n_dims_code\n",
        "\n",
        "        if self.training:\n",
        "            # Draw standard normal samples \"epsilon\"\n",
        "            # Use the reparameterization trick\n",
        "            eps_NC = torch.randn(N, C)\n",
        "            z_NC = mu_NC + eps_NC * self.q_sigma\n",
        "            return z_NC\n",
        "        else:\n",
        "            # For evaluations, we always just use the mean\n",
        "            return mu_NC\n",
        "\n",
        "    def encode(self, x_ND):\n",
        "        \"\"\"\n",
        "        Args\n",
        "        ----\n",
        "        x_ND: the observation vector\n",
        "        \"\"\"\n",
        "        return self.encoder.forward(x_ND)\n",
        "\n",
        "    def decode(self, z_NC):\n",
        "        \"\"\"\n",
        "        Args\n",
        "        ----\n",
        "        z_NC: the code vector\n",
        "        \"\"\"\n",
        "        return self.decoder.forward(z_NC)\n",
        "\n",
        "    def binary_predict_error_rate(self, f_predict, f_true):\n",
        "        \"\"\"\n",
        "        \n",
        "        \"\"\"\n",
        "        length = f_predict.size()[0]\n",
        "        f_predict_binary = (f_predict > 0.5).type(torch.FloatTensor)\n",
        "        error = torch.sum((f_predict_binary - f_true).abs_()) / length\n",
        "        return error\n",
        "\n",
        "    def calc_vi_loss(self, xs_ND, ys_ND, vals, n_mc_samples=1):\n",
        "        \"\"\"\n",
        "        Args\n",
        "\n",
        "        xs_ND: the input feature vectors\n",
        "        ys_ND: the other feature vectors in mini-batch\n",
        "        vals: the entry associated with (x,y)\n",
        "        n_mc_samples: \n",
        "\n",
        "        ----\n",
        "        Returns:\n",
        "        loss\n",
        "\n",
        "        \"\"\"\n",
        "        neg_expected_ll = 0.0\n",
        "        # Given a (potentially) a tensor of observation vectors, \n",
        "        # Encode it into latent space\n",
        "        mx_NC = self.encode(xs_ND)\n",
        "        my_NC = self.encode(ys_ND)\n",
        "        \n",
        "        # Compute the KL divergence\n",
        "        # KL(N(mx_NC, q_sigma) || N(0, I))\n",
        "        kl_xz_NC = -0.5 * torch.sum(1 + torch.log(self.q_sigma ** 2) - mx_NC ** 2 - self.q_sigma ** 2)\n",
        "        kl_yz_NC = -0.5 * torch.sum(1 + torch.log(self.q_sigma ** 2) - my_NC ** 2 - self.q_sigma ** 2)\n",
        "        kl       = kl_xz_NC + kl_yz_NC # Total KL term\n",
        "        \n",
        "        # Generate samples from N(mx_NC, q_sigma) to compute the following\n",
        "        # E_q[log p(x_ND|mx_NC)]\n",
        "        for ss in range(n_mc_samples):\n",
        "            sample_z_NC      = self.draw_sample_from_q(mx_NC)\n",
        "            sample_xproba_ND = self.decode(sample_z_NC)\n",
        "\n",
        "            # Use MSE to measure reconstruction loss\n",
        "            # Since MSE is equivalent to log gaussian loss\n",
        "            sample_mse_loss  = F.mse_loss(sample_xproba_ND, xs_ND)\n",
        "\n",
        "            # KL divergence from q(mu, sigma) to prior (std normal)\n",
        "            neg_expected_ll += 1/n_mc_samples * sample_mse_loss\n",
        "        \n",
        "        \n",
        "        # Compute the loss from adjacency matrix reconstruction\n",
        "        # Get number of entries\n",
        "        num_samples = len(vals)\n",
        "        f_predict   = torch.zeros(num_samples)\n",
        "\n",
        "        # Compute\n",
        "        # E_q[log p(A_ij|x_i, x_j)] = E_q[Bern(A_ij|sigmoid(x_i dot x_j))]\n",
        "        \n",
        "        for ss in range(n_mc_samples):\n",
        "            # These two should have the same shape\n",
        "            # which is (N*C)\n",
        "            sample_z_NC = self.draw_sample_from_q(mx_NC)\n",
        "            sample_y_NC = self.draw_sample_from_q(my_NC)\n",
        "\n",
        "            # inner_prod.shape = (N,)\n",
        "            inner_prod  = torch.sum(sample_z_NC * sample_y_NC, dim=1)\n",
        "            f_predict  += 1/n_mc_samples * torch.sigmoid(inner_prod)\n",
        "\n",
        "        # Use binary cross entry loss, NOTE that this is for\n",
        "        # adjacency matrix whose entry values are 0 and 1\n",
        "        # This will need to change for other types of adjacency matrix value\n",
        "        # Use the binary prediction loss with logits\n",
        "        matrix_reconstruction_loss = \\\n",
        "            F.binary_cross_entropy(f_predict, Variable(torch.FloatTensor(vals)), reduction='sum')\n",
        "        \n",
        "        neg_expected_ll += matrix_reconstruction_loss\n",
        "        \n",
        "        return neg_expected_ll, kl, matrix_reconstruction_loss, sample_xproba_ND\n",
        "      \n",
        "      \n",
        "\n",
        "class VariationalAutoencoderHSP(VariationalAutoencoder):\n",
        "    def __init__(self,inference_engine,**kwargs,):\n",
        "\n",
        "        super(VariationalAutoencoderHSP, self).__init__(**kwargs)\n",
        "        assert inference_engine\n",
        "        self.inference_engine = inference_engine\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qOLmcpS_0MJE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# VAE with horseshoe priors"
      ]
    },
    {
      "metadata": {
        "id": "1cydvYRB0OqL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\" Uses a non-centered parameterization of the model.\n",
        "    Fully factorized Gaussian + IGamma Variational distribution\n",
        "\tq = N(w_ijl | m_ijl, sigma^2_ijl) N(ln \\tau_kl | params) IGamma(\\lambda_kl| params)\n",
        "\tIGamma(\\tau_l | params) IGamma(\\lambda_l| params)\n",
        "\"\"\"\n",
        "\n",
        "import autograd.numpy as np\n",
        "import autograd.numpy.random as npr\n",
        "from autograd.scipy.misc import logsumexp\n",
        "from autograd.scipy.special import gammaln, psi\n",
        "from src.utility_functions import diag_gaussian_entropy, inv_gamma_entropy, log_normal_entropy\n",
        "\n",
        "\n",
        "class FactorizedHierarchicalInvGamma:\n",
        "    def __init__(self, n_weights, lambda_a, lambda_b, lambda_b_global, tau_a, shapes, train_stats, classification=False,\n",
        "                 n_data=None):\n",
        "        self.name = \"Factorized Hierarchical Inverse Gamma Variational Approximation\"\n",
        "        self.classification = classification\n",
        "        self.n_weights = n_weights\n",
        "        self.shapes = shapes\n",
        "        self.num_hidden_layers = len(shapes) - 1\n",
        "        self.lambda_a_prior = lambda_a\n",
        "        self.lambda_b_prior = lambda_b\n",
        "        self.lambda_a_prior_global = 0.5\n",
        "        self.lambda_b_prior_global = lambda_b_global\n",
        "        self.lambda_a_prior_oplayer = 0.5\n",
        "        self.lambda_b_prior_oplayer = 1.\n",
        "        self.tau_a_prior = tau_a\n",
        "        self.tau_a_prior_global = 0.5\n",
        "        self.tau_a_prior_oplayer = 0.5\n",
        "        self.l2pi = np.log(2 * np.pi)\n",
        "        self.n_data = n_data\n",
        "        self.noise_entropy = None\n",
        "        if not self.classification:\n",
        "            # gamma(6, 6) prior on precision\n",
        "            self.noise_a = 6.\n",
        "            self.noise_b = 6.\n",
        "            self.train_stats = train_stats\n",
        "\n",
        "    ######### PACK UNPACK PARAMS #################################################\n",
        "    def initialize_variational_params(self, param_scale=1):\n",
        "        # Initialize weights\n",
        "        wlist = list()\n",
        "        for m, n in self.shapes:\n",
        "            wlist.append(npr.randn(m * n) * np.sqrt(2 / m))\n",
        "            wlist.append(np.zeros(n))  # bias\n",
        "        w = np.concatenate(wlist)\n",
        "        log_sigma = param_scale * npr.randn(w.shape[0]) - 10.\n",
        "        # initialize scale parameters\n",
        "        self.tot_outputs = 0\n",
        "        for _, num_hl_outputs in self.shapes:\n",
        "            self.tot_outputs += num_hl_outputs\n",
        "        # No hs priors on the outputs\n",
        "        self.tot_outputs = self.tot_outputs - self.shapes[-1][1]\n",
        "        if not self.classification:\n",
        "            tau_mu, tau_log_sigma, tau_global_mu, tau_global_log_sigma, tau_oplayer_mu, tau_oplayer_log_sigma, log_a, \\\n",
        "                                log_b = self.initialize_scale_from_prior()\n",
        "            init_params = np.concatenate([w.ravel(), log_sigma.ravel(),\n",
        "                                          tau_mu.ravel(), tau_log_sigma.ravel(), tau_global_mu.ravel(),\n",
        "                                          tau_global_log_sigma.ravel(), tau_oplayer_mu, tau_oplayer_log_sigma, log_a,\n",
        "                                          log_b])\n",
        "        else:\n",
        "            tau_mu, tau_log_sigma, tau_global_mu, tau_global_log_sigma, tau_oplayer_mu, tau_oplayer_log_sigma = \\\n",
        "                self.initialize_scale_from_prior()\n",
        "            init_params = np.concatenate([w.ravel(), log_sigma.ravel(),\n",
        "                                          tau_mu.ravel(), tau_log_sigma.ravel(), tau_global_mu.ravel(),\n",
        "                                          tau_global_log_sigma.ravel(), tau_oplayer_mu, tau_oplayer_log_sigma])\n",
        "\n",
        "        return init_params\n",
        "\n",
        "    def initialize_scale_from_prior(self):\n",
        "        # scale parameters (hidden + observed),\n",
        "        self.lambda_a_hat = (self.tau_a_prior + self.lambda_a_prior) * np.ones([self.tot_outputs, 1]).ravel()\n",
        "        self.lambda_b_hat = (1.0 / self.lambda_b_prior ** 2) * np.ones([self.tot_outputs, 1]).ravel()\n",
        "        self.lambda_a_hat_global = (self.tau_a_prior_global + self.lambda_a_prior_global)  \\\n",
        "            * np.ones([self.num_hidden_layers, 1]).ravel()\n",
        "        self.lambda_b_hat_global = (1.0 / self.lambda_b_prior_global ** 2) * np.ones(\n",
        "            [self.num_hidden_layers, 1]).ravel()\n",
        "        # set oplayer lambda param\n",
        "        self.lambda_a_hat_oplayer = np.array(self.tau_a_prior_oplayer + self.lambda_a_prior_oplayer).reshape(-1)\n",
        "        self.lambda_b_hat_oplayer = (1.0 / self.lambda_b_prior_oplayer ** 2) * np.ones([1]).ravel()\n",
        "        # sample from half cauchy and log to initialize the mean of the log normal\n",
        "        sample = np.abs(self.lambda_b_prior * (npr.randn(self.tot_outputs) / npr.randn(self.tot_outputs)))\n",
        "        tau_mu = np.log(sample)\n",
        "        tau_log_sigma = npr.randn(self.tot_outputs) - 10.\n",
        "        # one tau_global for each hidden layer\n",
        "        sample = np.abs(\n",
        "            self.lambda_b_prior_global * (npr.randn(self.num_hidden_layers) / npr.randn(self.num_hidden_layers)))\n",
        "        tau_global_mu = np.log(sample)\n",
        "        tau_global_log_sigma = npr.randn(self.num_hidden_layers) - 10.\n",
        "        # one tau for all op layer weights\n",
        "        sample = np.abs(self.lambda_b_hat_oplayer * (npr.randn() / npr.randn()))\n",
        "        tau_oplayer_mu = np.log(sample)\n",
        "        tau_oplayer_log_sigma = npr.randn(1) - 10.\n",
        "        if not self.classification:\n",
        "            log_a = np.array(np.log(self.noise_a)).reshape(-1)\n",
        "            log_b = np.array(np.log(self.noise_b)).reshape(-1)\n",
        "            return tau_mu, tau_log_sigma, tau_global_mu, tau_global_log_sigma, tau_oplayer_mu, tau_oplayer_log_sigma, \\\n",
        "                   log_a, log_b\n",
        "        else:\n",
        "            return tau_mu, tau_log_sigma, tau_global_mu, tau_global_log_sigma, tau_oplayer_mu, tau_oplayer_log_sigma\n",
        "\n",
        "    def unpack_params(self, params):\n",
        "        # unpack params\n",
        "        w_vect = params[:self.n_weights]\n",
        "        num_std = 2 * self.n_weights\n",
        "        sigma = np.log(1 + np.exp(params[self.n_weights:num_std]))\n",
        "        tau_mu = params[num_std:num_std + self.tot_outputs]\n",
        "        tau_sigma = np.log(\n",
        "            1 + np.exp(params[num_std + self.tot_outputs:num_std + 2 * self.tot_outputs]))\n",
        "        tau_mu_global = params[num_std + 2 * self.tot_outputs: num_std + 2 * self.tot_outputs + self.num_hidden_layers]\n",
        "        tau_sigma_global = np.log(1 + np.exp(params[num_std + 2 * self.tot_outputs + self.num_hidden_layers:num_std +\n",
        "                                                                    2 * self.tot_outputs + 2 * self.num_hidden_layers]))\n",
        "        tau_mu_oplayer = params[num_std + 2 * self.tot_outputs + 2 * self.num_hidden_layers: num_std +\n",
        "                                                                2 * self.tot_outputs + 2 * self.num_hidden_layers + 1]\n",
        "        tau_sigma_oplayer = np.log(\n",
        "            1 + np.exp(params[num_std + 2 * self.tot_outputs + 2 * self.num_hidden_layers + 1:]))\n",
        "        if not self.classification:\n",
        "            a = tau_sigma_oplayer[1]\n",
        "            b = tau_sigma_oplayer[2]\n",
        "            tau_sigma_oplayer = tau_sigma_oplayer[0]\n",
        "            egamma = a / b\n",
        "            elog_gamma = psi(a) - np.log(b)\n",
        "            self.noise_entropy = inv_gamma_entropy(a, b)\n",
        "            #  we will just use a point estimate of noise_var b/a+1 (noise_var ~ IGamma) for computing predictive ll\n",
        "            self.noisevar = (b / (a + 1)) * self.train_stats['sigma'] ** 2\n",
        "            return w_vect, sigma, tau_mu, tau_sigma, tau_mu_global, tau_sigma_global, tau_mu_oplayer, \\\n",
        "                   tau_sigma_oplayer, elog_gamma, egamma\n",
        "        else:\n",
        "            return w_vect, sigma, tau_mu, tau_sigma, tau_mu_global, tau_sigma_global, tau_mu_oplayer, tau_sigma_oplayer\n",
        "\n",
        "    def unpack_layer_weight_variances(self, sigma_vect):\n",
        "        for m, n in self.shapes:\n",
        "            yield sigma_vect[:m * n].reshape((m, n)), sigma_vect[m * n:m * n + n]\n",
        "            sigma_vect = sigma_vect[(m + 1) * n:]\n",
        "\n",
        "    def unpack_layer_weight_priors(self, tau_vect):\n",
        "        for m, n in self.shapes:\n",
        "            yield tau_vect[:n]\n",
        "            tau_vect = tau_vect[n:]\n",
        "\n",
        "    def unpack_layer_weights(self, w_vect):\n",
        "        for m, n in self.shapes:\n",
        "            yield w_vect[:m * n].reshape((m, n)), w_vect[m * n:m * n + n]\n",
        "            w_vect = w_vect[(m + 1) * n:]\n",
        "\n",
        "    ######### Fixed Point Updates ################################## #####\n",
        "    def fixed_point_updates(self, params):\n",
        "        if self.classification:\n",
        "            w_vect, sigma, tau_mu, tau_sigma, tau_mu_global, tau_sigma_global, tau_mu_oplayer, tau_sigma_oplayer = \\\n",
        "                self.unpack_params(params)\n",
        "        else:\n",
        "            w_vect, sigma, tau_mu, tau_sigma, tau_mu_global, tau_sigma_global, tau_mu_oplayer, tau_sigma_oplayer, _, _ \\\n",
        "                = self.unpack_params(params)\n",
        "        # update lambda moments\n",
        "        self.lambda_b_hat = np.exp(-tau_mu + 0.5 * tau_sigma ** 2) + (1. / self.lambda_b_prior ** 2)\n",
        "        self.lambda_b_hat_global = np.exp(-tau_mu_global + 0.5 * tau_sigma_global ** 2) + (\n",
        "            1. / self.lambda_b_prior_global ** 2)\n",
        "        self.lambda_b_hat_oplayer = np.exp(-tau_mu_oplayer + 0.5 * tau_sigma_oplayer ** 2) + (\n",
        "            1. / self.lambda_b_prior_oplayer ** 2)\n",
        "        return None\n",
        "\n",
        "    ######### ELBO CALC ################################################\n",
        "    def lrpm_forward_pass(self, mu_vect, sigma_vect, tau_mu_vect, tau_sigma_vect, tau_mu_global, tau_sigma_global,\n",
        "                          tau_mu_oplayer, tau_sigma_oplayer, inputs):\n",
        "        for layer_id, (mu, var, tau_mu, tau_sigma) in enumerate(\n",
        "                zip(self.unpack_layer_weights(mu_vect), self.unpack_layer_weight_variances(sigma_vect),\n",
        "                    self.unpack_layer_weight_priors(tau_mu_vect),\n",
        "                    self.unpack_layer_weight_priors(tau_sigma_vect))):\n",
        "            w, b = mu\n",
        "            sigma__w, sigma_b = var\n",
        "            if layer_id < len(self.shapes) - 1:\n",
        "                scale_mu = 0.5 * (tau_mu + tau_mu_global[layer_id])\n",
        "                scale_v = 0.25 * (tau_sigma ** 2 + tau_sigma_global[layer_id] ** 2)\n",
        "                scale = np.exp(scale_mu + np.sqrt(scale_v) * npr.randn(tau_mu.shape[0]))\n",
        "                mu_w = np.dot(inputs, w) + b\n",
        "                v_w = np.dot(inputs ** 2, sigma__w ** 2) + sigma_b ** 2\n",
        "                outputs = (np.sqrt(v_w) / np.sqrt(inputs.shape[1])) * np.random.normal(size=mu_w.shape) + mu_w\n",
        "                outputs = scale * outputs\n",
        "                inputs = outputs * (outputs > 0)\n",
        "            else:\n",
        "                op_scale_mu = 0.5 * tau_mu_oplayer\n",
        "                op_scale_v = 0.25 * tau_sigma_oplayer ** 2\n",
        "                Ekappa_half = np.exp(op_scale_mu + np.sqrt(op_scale_v) * npr.randn())\n",
        "                mu_w = np.dot(inputs, w) + b\n",
        "                v_w = np.dot(inputs ** 2, sigma__w ** 2) + sigma_b ** 2\n",
        "                outputs = Ekappa_half * (np.sqrt(v_w) / np.sqrt(inputs.shape[1])) * np.random.normal(\n",
        "                    size=mu_w.shape) + mu_w\n",
        "        return outputs\n",
        "\n",
        "    def EPw_Gaussian(self, prior_precision, w, sigma):\n",
        "        \"\"\"\"\\int q(z) log p(z) dz, assuming gaussian q(z) and p(z)\"\"\"\n",
        "        wD = w.shape[0]\n",
        "        prior_wvar_ = 1. / prior_precision\n",
        "        a = - 0.5 * wD * np.log(2 * np.pi) - 0.5 * wD * np.log(prior_wvar_) - 0.5 * prior_precision * (\n",
        "            np.dot(w.T, w) + np.sum((sigma ** 2)))\n",
        "        return a\n",
        "\n",
        "    def EP_Gamma(self, Egamma, Elog_gamma):\n",
        "        \"\"\" Enoise precision \"\"\"\n",
        "        return self.noise_a * np.log(self.noise_b) - gammaln(self.noise_a) + (\n",
        "                                                            - self.noise_a - 1) * Elog_gamma - self.noise_b * Egamma\n",
        "\n",
        "    def EPtaulambda(self, tau_mu, tau_sigma, tau_a_prior, lambda_a_prior,\n",
        "                    lambda_b_prior, lambda_a_hat, lambda_b_hat):\n",
        "        \"\"\" E[ln p(\\tau | \\lambda)] + E[ln p(\\lambda)]\"\"\"\n",
        "        etau_given_lambda = -gammaln(tau_a_prior) - tau_a_prior * (np.log(lambda_b_hat) - psi(lambda_a_hat)) + (\n",
        "                            -tau_a_prior - 1.) * tau_mu - np.exp(-tau_mu + 0.5 * tau_sigma ** 2) * (lambda_a_hat /\n",
        "                                               lambda_b_hat)\n",
        "        elambda = -gammaln(lambda_a_prior) - 2 * lambda_a_prior * np.log(lambda_b_prior) + (-lambda_a_prior - 1.) * (\n",
        "            np.log(lambda_b_hat) - psi(lambda_a_hat)) - (1. / lambda_b_prior ** 2) * (lambda_a_hat / lambda_b_hat)\n",
        "        return np.sum(etau_given_lambda) + np.sum(elambda)\n",
        "\n",
        "    def entropy(self, sigma, tau_sigma, tau_mu, tau_sigma_global, tau_mu_global, tau_sigma_oplayer, tau_mu_oplayer):\n",
        "        ent_w = diag_gaussian_entropy(np.log(sigma), self.n_weights)\n",
        "        ent_tau = log_normal_entropy(np.log(tau_sigma), tau_mu, self.tot_outputs) + log_normal_entropy(\n",
        "            np.log(tau_sigma_global), tau_mu_global, self.num_hidden_layers) + log_normal_entropy(\n",
        "            np.log(tau_sigma_oplayer), tau_mu_oplayer, 1)\n",
        "        ent_lambda = inv_gamma_entropy(self.lambda_a_hat, self.lambda_b_hat) + inv_gamma_entropy(\n",
        "            self.lambda_a_hat_global, self.lambda_b_hat_global) + inv_gamma_entropy(self.lambda_a_hat_oplayer,\n",
        "                                                                                    self.lambda_b_hat_oplayer)\n",
        "        return ent_w, ent_tau, ent_lambda\n",
        "\n",
        "    def compute_elbo_contribs(self, params, x, y):\n",
        "        if self.classification:\n",
        "            w_vect, sigma, tau_mu, tau_sigma, tau_mu_global, tau_sigma_global, tau_mu_oplayer, tau_sigma_oplayer \\\n",
        "                = self.unpack_params(params)\n",
        "        else:\n",
        "            w_vect, sigma, tau_mu, tau_sigma, tau_mu_global, tau_sigma_global, tau_mu_oplayer, tau_sigma_oplayer, \\\n",
        "            Elog_gamma, Egamma = self.unpack_params(params)\n",
        "        preds = self.lrpm_forward_pass(w_vect, sigma, tau_mu, tau_sigma,\n",
        "                                       tau_mu_global, tau_sigma_global, tau_mu_oplayer, tau_sigma_oplayer, x)\n",
        "        if self.classification:\n",
        "            preds = preds - logsumexp(preds, axis=1, keepdims=True)\n",
        "            log_lik = np.sum(np.sum(y * preds, axis=1), axis=0)\n",
        "        else:\n",
        "            log_lik = -0.5 * np.sum((preds - y.reshape(-1, 1)) ** 2) * Egamma - 0.5 * preds.shape[0] * self.l2pi \\\n",
        "                      + 0.5 * preds.shape[0] * Elog_gamma\n",
        "\n",
        "        log_prior = self.EPw_Gaussian(1., w_vect, sigma)\n",
        "        log_prior = log_prior + \\\n",
        "                    self.EPtaulambda(tau_mu, tau_sigma, self.tau_a_prior, self.lambda_a_prior, self.lambda_b_prior,\n",
        "                                     self.lambda_a_hat, self.lambda_b_hat) + \\\n",
        "                    self.EPtaulambda(tau_mu_global, tau_sigma_global, self.tau_a_prior_global,\n",
        "                                     self.lambda_a_prior_global, self.lambda_b_prior_global, self.lambda_a_hat_global,\n",
        "                                     self.lambda_b_hat_global) + \\\n",
        "                    self.EPtaulambda(tau_mu_oplayer, tau_sigma_oplayer, self.tau_a_prior_oplayer,\n",
        "                                     self.lambda_a_prior_oplayer, self.lambda_b_prior_oplayer,\n",
        "                                     self.lambda_a_hat_oplayer, self.lambda_b_hat_oplayer)\n",
        "        ent_w, ent_tau, ent_lambda = self.entropy(sigma, tau_sigma, tau_mu, tau_sigma_global, tau_mu_global,\n",
        "                                                  tau_sigma_oplayer, tau_mu_oplayer)\n",
        "\n",
        "        if not self.classification:\n",
        "            log_prior = log_prior + self.EP_Gamma(Egamma, Elog_gamma)\n",
        "            ent_lambda = ent_lambda + self.noise_entropy  # hack add it to lambda entropy\n",
        "        return log_lik, log_prior, ent_w, ent_tau, ent_lambda\n",
        "\n",
        "\n",
        "    def compute_train_err(self, params, X, y):\n",
        "        if self.classification:\n",
        "            W_vect, sigma, tau_mu, tau_sigma, tau_mu_global, tau_sigma_global, tau_mu_oplayer, tau_sigma_oplayer = \\\n",
        "                self.unpack_params(params)\n",
        "        else:\n",
        "            W_vect, sigma, tau_mu, tau_sigma, tau_mu_global, tau_sigma_global, tau_mu_oplayer, tau_sigma_oplayer, _, _ \\\n",
        "                = self.unpack_params(params)\n",
        "        preds = self.lrpm_forward_pass(W_vect, sigma, tau_mu, tau_sigma, tau_mu_global, tau_sigma_global,\n",
        "                                       tau_mu_oplayer, tau_sigma_oplayer, X)\n",
        "        if self.classification:\n",
        "            preds = np.exp(preds - logsumexp(preds, axis=1, keepdims=True))\n",
        "            tru_labels = np.argmax(y, axis=1)\n",
        "            pred_labels = np.argmax(preds, axis=1)\n",
        "            err_ids = tru_labels != pred_labels\n",
        "            return 1. * np.sum(err_ids) / y.shape[0]\n",
        "        else:\n",
        "            return np.sqrt(np.mean((preds - y.reshape(-1, 1)) ** 2))\n",
        "\n",
        "    def compute_test_ll(self, params, x, y_test, num_samples=1):\n",
        "        if self.classification:\n",
        "            W_vect, sigma, tau_mu, tau_sigma, tau_mu_global, tau_sigma_global, tau_mu_oplayer, tau_sigma_oplayer = \\\n",
        "                self.unpack_params(params)\n",
        "        else:\n",
        "            W_vect, sigma, tau_mu, tau_sigma, tau_mu_global, tau_sigma_global, tau_mu_oplayer, tau_sigma_oplayer, \\\n",
        "            Elog_gamma, Egamma = self.unpack_params(params)\n",
        "        err_rate = 0.\n",
        "        test_ll = np.zeros([num_samples, y_test.shape[0]])\n",
        "        test_ll_dict = dict()\n",
        "        for i in np.arange(num_samples):\n",
        "            y = self.lrpm_forward_pass(W_vect, sigma, tau_mu, tau_sigma, tau_mu_global, tau_sigma_global,\n",
        "                                       tau_mu_oplayer, tau_sigma_oplayer, x)\n",
        "            if y_test.ndim == 1:\n",
        "                y = y.ravel()\n",
        "            if self.classification:\n",
        "                yraw = y - logsumexp(y, axis=1, keepdims=True)\n",
        "                y = np.exp(yraw)\n",
        "                tru_labels = np.argmax(y_test, axis=1)\n",
        "                pred_labels = np.argmax(y, axis=1)\n",
        "                err_ids = tru_labels != pred_labels\n",
        "                err_rate = err_rate + np.sum(err_ids) / y_test.shape[0]\n",
        "                # test_ll is scaled by number of test_points\n",
        "                test_ll[i] = np.mean(np.sum(y_test * np.log(y + 1e-32), axis=1))\n",
        "            else:\n",
        "                # scale by target stats\n",
        "                y_scaled = y * self.train_stats['sigma'] + self.train_stats['mu']\n",
        "                # rmse\n",
        "                err_rate = err_rate + np.sqrt(np.mean((y_test - y_scaled) ** 2))\n",
        "                test_ll[i] = (-0.5 * (1. / self.noisevar) * (y_test - y_scaled) ** 2 - 0.5 * self.l2pi - 0.5 * np.log(\n",
        "                    self.noisevar)).ravel()\n",
        "\n",
        "        err_rate = err_rate / num_samples\n",
        "        test_ll_dict['mu'] = np.mean(logsumexp(test_ll, axis=0) - np.log(num_samples))\n",
        "        return test_ll_dict, err_rate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E72JYLr6TbU8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Synthetic data generation\n",
        "\n",
        "Synthetic data (and real life data to test on) should include\n",
        "* Adjancency matrix (which might be sparse) where A_{ij} = 1 if there is an edge and 0 otherwise between two nodes i and j\n",
        "* The feature vectors for all the nodes"
      ]
    },
    {
      "metadata": {
        "id": "S8h-eAoXKYTu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_entry_values(coordinates, latent_vectors):\n",
        "    \"\"\"\n",
        "    coordinates: list of coordinates\n",
        "    \n",
        "    \"\"\"\n",
        "    vals    = list()\n",
        "    for i, j in coordinates:\n",
        "        if np.sum(latent_vectors[i] * latent_vectors[j]) > 0:\n",
        "            vals.append(1)\n",
        "        else:\n",
        "            vals.append(0)\n",
        "    return vals\n",
        "    \n",
        "\n",
        "def create_sparse_matrix_list(observed_coordinates, num_nodes, latent_vectors):\n",
        "    \"\"\"\n",
        "    observed_coordinates: list of (row,col) coordinates\n",
        "    num_nodes : int, number of nodes\n",
        "    latent_vectors : latent vectos whose inner products used to determine adjacency\n",
        "    matrix\n",
        "    \n",
        "    ---\n",
        "    return the sparse representation of the adjacency matrix\n",
        "    \n",
        "    \"\"\"\n",
        "    sparse_adjacency_matrix = [[] for _ in range(num_nodes)]\n",
        "\n",
        "    for entry in observed_coordinates:\n",
        "        idx, idy = entry[0], entry[1]\n",
        "        inner = np.dot(latent_vectors[idx, :], latent_vectors[idy, :])\n",
        "        val = 0\n",
        "        if inner > 0:\n",
        "            val = 1\n",
        "\n",
        "        sparse_adjacency_matrix[idx].append((entry, val))\n",
        "        sparse_adjacency_matrix[idy].append(([idy, idx], val))\n",
        "\n",
        "    return sparse_adjacency_matrix\n",
        "\n",
        "\n",
        "def create_observed_features(latent_vectors, num_nodes, true_dim, observed_dim, noise_feature_num=0):\n",
        "    \"\"\"    \n",
        "    latent_vectors : shape(num_nodes, true_dim), the latent feature vectors of all nodes\n",
        "    num_nodes : int, number of nodes in this network\n",
        "    true_dim  : int, dimension of the latent feature (the code)\n",
        "    observed_dim : int, dimension of the observed feature vector\n",
        "    num_noisy_dim : int, added noisy dimension\n",
        "    \n",
        "    ---\n",
        "    \n",
        "    Create observed features that contain both true and noisy features\n",
        "\n",
        "    1. Create a random transformation matrix A\n",
        "    2. Apply the transformation A X_l where X_l is the latent feature vectors\n",
        "    3. \n",
        "    ---\n",
        "    returns\n",
        "    \n",
        "    augmented_feature_matrix : shape(num_nodes, observed_dim + num_noisy_dim)\n",
        "    the augmented observed feature vector for all the nodes, with relevent\n",
        "    dimensions together with noisy entries.\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    # Create a random transformation, A\n",
        "    transformation_matrix = np.random.randn(observed_dim, true_dim)\n",
        "\n",
        "    # X_true = dot(A, x_true)\n",
        "    transformed_feature = np.dot(transformation_matrix, latent_vectors.T)\n",
        "    transformed_feature = transformed_feature.T\n",
        "    transformed_feature /= transformed_feature.max()  # Scale the feature vectors by dividing by the max value\n",
        "\n",
        "    # If no noisy feature, just return the transformed feature\n",
        "    if noise_feature_num == 0:\n",
        "        return transformed_feature\n",
        "    \n",
        "    # If there are noisy features, generate these from standard normal distribution\n",
        "    # Noise feature = N(0, I)\n",
        "    # Create noise matrix X_noise\n",
        "    noise_feature_matrix = np.random.randn(num_nodes, noise_feature_num)\n",
        "    augmented_dim = observed_dim + noise_feature_num\n",
        "\n",
        "    # Horizontally concatenate X_true :: X_noise\n",
        "    augmented_feature_matrix = np.hstack((transformed_feature, noise_feature_matrix))\n",
        "\n",
        "    # Permute the features column\n",
        "    augmented_feature_matrix = augmented_feature_matrix[:, np.random.permutation(augmented_dim)]\n",
        "    return augmented_feature_matrix\n",
        "\n",
        "\n",
        "def create_synthetic_data(num_nodes, sparsity, true_dim, observed_dim, num_noisy_dim):\n",
        "    \"\"\"\n",
        "    num_nodes : int, number of nodes in this network\n",
        "    sparsity  : float, ratio of all entries, which are observed\n",
        "    true_dim  : int, dimension of the latent feature (the code)\n",
        "    observed_dim : int, dimension of the observed feature vector\n",
        "    num_noisy_dim : int, added noisy dimension\n",
        "    \n",
        "    --- \n",
        "    returns\n",
        "    \n",
        "    (latent_vectors, observed_feature_vectors, sparse_adjancency_matrix)\n",
        "    \n",
        "    latent_vectors : shape(num_nodes, true_dim), all the latent feature vectors\n",
        "    observed_feature_vectors : shape(num_nodes, observed_dim), all the observed feature vectors\n",
        "    sparse_adjancency_matrix : list[list(entry, value)] \n",
        "    \n",
        "    \"\"\"\n",
        "    # Create num_nodes hidden vector randomly\n",
        "    latent_vectors = np.random.multivariate_normal(np.zeros((true_dim,)), \\\n",
        "                                                   np.eye(true_dim), \\\n",
        "                                                   size=(num_nodes,))\n",
        "\n",
        "    coordinates = [[x,y] for x in range(num_nodes) for y in range(x+1, num_nodes)]\n",
        "    total_num_pairs = len(coordinates)\n",
        "    num_observed = int(len(coordinates) * sparsity)\n",
        "\n",
        "    # Pick a number of random coordinates\n",
        "    observed_idx = np.random.choice(total_num_pairs, num_observed, replace=False)\n",
        "    observed_coordinates = np.take(coordinates, observed_idx, axis=0)\n",
        "    \n",
        "    train_size = int(len(observed_coordinates) * 0.8)\n",
        "    train_coordinates = observed_coordinates[:train_size]\n",
        "    test_coordinates = observed_coordinates[train_size+1:]\n",
        "    \n",
        "    # Create sparse matrix representation\n",
        "    train_sparse_adjacency_matrix = create_sparse_matrix_list(train_coordinates, num_nodes, latent_vectors)\n",
        "    test_sparse_adjacency_matrix = create_sparse_matrix_list(test_coordinates, num_nodes, latent_vectors)\n",
        "    \n",
        "    # Create test matrix and train matrix\n",
        "    observed_feature_vectors = create_observed_features(latent_vectors, num_nodes, true_dim, observed_dim,\n",
        "                                                        num_noisy_dim)\n",
        "\n",
        "    return latent_vectors, observed_feature_vectors, train_sparse_adjacency_matrix, test_sparse_adjacency_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vr8CH4BrKL6v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Do training\n",
        "\n",
        "TODO:\n",
        "* Update the training function, first generate synthetic data, organize into batches\n",
        "* Compare between round-robin updates (aka update each row by row where the list of observations always share a common row index) vs mini-batch update where the list of observations can contain any random pair of two indices.\n"
      ]
    },
    {
      "metadata": {
        "id": "_xDkRxRa3mI4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Evaluation function"
      ]
    },
    {
      "metadata": {
        "id": "jBpBZKEVQuiX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to do evaluation\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "def classification_accuracy(model, feature_tensor, adjacency_matrix, report=False):\n",
        "    \"\"\"\n",
        "    Feature vectors.shape -> (num_nodes, number of observed features)\n",
        "    adjancency_matrix -> sparse representation of adjancey matrix\n",
        "    where each row -> list of (coordinates, value) associated with the specific\n",
        "    factor\n",
        "\n",
        "    Uses model.encode(feature_vector)\n",
        "    \"\"\"\n",
        "    latent_vectors = model.encode(feature_tensor)\n",
        "    latent_adj_mat = torch.mm(latent_vectors, latent_vectors.transpose(0 , 1))\n",
        "    num_accurate = 0.0\n",
        "    num_observed = 0.0\n",
        "    for row in adjacency_matrix:\n",
        "        for coor, val in row:\n",
        "            if (\n",
        "                                latent_adj_mat[coor[0]][coor[1]].item() < 0.0 and val == 0.0 or\n",
        "                                latent_adj_mat[coor[0]][coor[1]].item() >= 0.0 and val == 1.0\n",
        "            ):\n",
        "\n",
        "                if report:\n",
        "                    print(\"predict: \", latent_adj_mat[coor[0]][coor[1]].item(), \" val: \", val)\n",
        "\n",
        "                num_accurate += 1\n",
        "            num_observed += 1\n",
        "\n",
        "    return num_accurate/ num_observed\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bt80bWYI3pHq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training"
      ]
    },
    {
      "metadata": {
        "id": "EJ4zwglopm5E",
        "colab_type": "code",
        "outputId": "068e5b21-5e8c-4ed0-d41e-5144e69d66ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "def train_on_synthetic_data(model, num_nodes, observed_dim, true_dim, fake_dim):\n",
        "    sparsity = 0.25\n",
        "    true_vectors, feature_vectors, train_adjacency_matrix, test_adjacency_matrix\\\n",
        "                = create_synthetic_data(num_nodes, sparsity, true_dim, observed_dim,\n",
        "                                                                            fake_dim)\n",
        "\n",
        "    # NOTE that there exists the 'interchangeability problem'\n",
        "    # XR^TRX = X^TX\n",
        "    # Y = XR^T\n",
        "    \n",
        "    # Initialize optimizer\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    obj_losses = []\n",
        "    epochs = []\n",
        "    all_epochs = []\n",
        "\n",
        "    for epoch in range(1001): \n",
        "        # Do round-robin optimization\n",
        "        for idx in range(num_nodes):\n",
        "            x_ND = Variable(torch.FloatTensor([feature_vectors[idx, :]]))\n",
        "            ys_ND = list()\n",
        "            observed_entries = train_adjacency_matrix[idx]\n",
        "            other_vec_idx = [entry[0][1] for entry in observed_entries]\n",
        "            vals = [entry[1] for entry in observed_entries]\n",
        "\n",
        "            for idy in other_vec_idx:\n",
        "                ys_ND.append(feature_vectors[idy, :])\n",
        "\n",
        "            ys_ND = Variable(torch.FloatTensor(ys_ND))\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # NOTE:\n",
        "            # expected_ll refers to the expected log likelihood term of ELBO\n",
        "            # kl refers to the KL divergence term of the ELBO\n",
        "            # matrix_loss refers to the matrix reconstruction loss\n",
        "            neg_expected_ll, KL, matrix_loss, _ = model.calc_vi_loss(x_ND, ys_ND, vals, n_mc_samples=10)\n",
        "            \n",
        "            KL = 1/len(observed_entries) * KL\n",
        "            # TODO: scale the KL term\n",
        "            # ELBO loss = negative expected log likelihood + KL\n",
        "            elbo_loss = neg_expected_ll + KL\n",
        "            \n",
        "            elbo_loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "        if epoch % 10 == 0:\n",
        "            all_epochs.append(epoch)\n",
        "            obj_losses.append(elbo_loss)\n",
        "            \n",
        "\n",
        "        if epoch in [0, 1, 25] or epoch % 50 == 0:\n",
        "            all_vectors = Variable(torch.FloatTensor(feature_vectors))\n",
        "            train_accuracy = classification_accuracy(model, all_vectors, train_adjacency_matrix)\n",
        "            test_accuracy = classification_accuracy(model, all_vectors, test_adjacency_matrix)\n",
        "            epochs.append(epoch)\n",
        "            train_losses.append(train_accuracy)\n",
        "            test_losses.append(test_accuracy)\n",
        "            \n",
        "            print(\"epoch: \", epoch, \" - objective loss: \", np.around(elbo_loss.data.item(),4), \" - train accuracy: \",\n",
        "                  np.around(train_accuracy,4), \" - test accuracy: \", np.around(test_accuracy, 4))\n",
        "\n",
        "\n",
        "    plt.plot(epochs, train_losses, '-', color='b', label='Link predict accuracy on train data')\n",
        "    plt.plot(epochs, test_losses, '--', color='r', label='Link predict accuracy on test data')\n",
        "    plt.ylim((0, 1))\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "    plt.plot(all_epochs, obj_losses, '-', color='r', label='ELBO loss')\n",
        "    plt.ylabel('ELBO value')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.show()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "num_nodes = 50\n",
        "observed_dim = 15\n",
        "true_dim = 3\n",
        "num_fake_dim = 1\n",
        "\n",
        "hidden_layer_sizes = [300]\n",
        "naive_model = VariationalAutoencoder(n_dims_code=5, \\\n",
        "                               n_dims_data=observed_dim+num_fake_dim, \\\n",
        "                               hidden_layer_sizes=hidden_layer_sizes)\n",
        "\n",
        "train_on_synthetic_data(naive_model, num_nodes, observed_dim, true_dim, num_fake_dim)\n",
        "\n",
        "# inference_engine = FactorizedHierarchicalInvGamma\n",
        "# hsbnn_model = VariationalAutoencoderHSP(inference_engine,\n",
        "#                                n_dims_code=5, \\\n",
        "#                                n_dims_data=observed_dim+num_fake_dim, \\\n",
        "#                                hidden_layer_sizes=hidden_layer_sizes)\n",
        "# train_on_synthetic_data(hsbnn_model, num_nodes, observed_dim, true_dim, num_fake_dim)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:  0  - objective loss:  13.62  - train accuracy:  0.8115  - test accuracy:  0.7705\n",
            "epoch:  1  - objective loss:  12.2619  - train accuracy:  0.832  - test accuracy:  0.7541\n",
            "epoch:  25  - objective loss:  10.379  - train accuracy:  0.9795  - test accuracy:  0.8525\n",
            "epoch:  50  - objective loss:  10.3503  - train accuracy:  0.9836  - test accuracy:  0.8197\n",
            "epoch:  100  - objective loss:  10.273  - train accuracy:  0.9918  - test accuracy:  0.8197\n",
            "epoch:  150  - objective loss:  10.3947  - train accuracy:  1.0  - test accuracy:  0.8197\n",
            "epoch:  200  - objective loss:  10.1204  - train accuracy:  1.0  - test accuracy:  0.8033\n",
            "epoch:  250  - objective loss:  9.8577  - train accuracy:  1.0  - test accuracy:  0.7869\n",
            "epoch:  300  - objective loss:  9.9567  - train accuracy:  1.0  - test accuracy:  0.7869\n",
            "epoch:  350  - objective loss:  10.0222  - train accuracy:  1.0  - test accuracy:  0.7869\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EgmGFYj9CM7p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "FactorizedHierarchicalInvGamma class, taken from https://github.com/dtak/hs-bnn-public/blob/master/src/factorized_approximation.py\n",
        "\n",
        "TODO: More detailed description on why this is needed"
      ]
    }
  ]
}