{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Horseshoe-VAE.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "-fsaZQ4qqUxL",
        "LiB6j9qcB4L7",
        "lDDvGLPR69XB",
        "WtqVJIMR_-28",
        "IdhSSbsHzrsB",
        "E72JYLr6TbU8",
        "KPMzGtTg7_Mp",
        "OR95gUPiv3Aq",
        "_xDkRxRa3mI4",
        "bt80bWYI3pHq"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dnguyen1196/Horseshoe-VAE/blob/master/Horseshoe_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "1tL3zPeo7h8I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 0. Set up"
      ]
    },
    {
      "metadata": {
        "id": "-fsaZQ4qqUxL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download KEGG file from GGdrive\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "D-GE8kpfgHoI",
        "colab_type": "code",
        "outputId": "4b4270ff-0c81-499d-f5a7-c4bf2141a95c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "cell_type": "code",
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kmDCJqexrWNu",
        "colab_type": "code",
        "outputId": "79b25f11-f782-4c01-ca65-754520c9ddba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "G = nx.read_gpickle(\"/content/drive/My Drive/BDL_project/kegg.ungraph.pkl\")\n",
        "\n",
        "for n in G.nodes:\n",
        "    print(\"Fingerprints for node %s: %r\" % (n, G.nodes[n][\"fingerprint\"]))\n",
        "    break\n",
        "    \n",
        "for e in G.edges:\n",
        "    print(e)\n",
        "    break"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fingerprints for node C00013: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
            "       0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
            "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
            "       0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0])\n",
            "('C00013', 'C00009')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LiB6j9qcB4L7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Install torch and autograd\n",
        "\n",
        "NOTE:\n",
        "Compared to the reference VAE implementation, the main difference is in the calc_vi_loss function\n",
        "where the likelihood term has been augmented with the loss term to reconstruct the adjacency matrix\n"
      ]
    },
    {
      "metadata": {
        "id": "jHLhxy06GNQM",
        "colab_type": "code",
        "outputId": "27528b5c-1a64-4ba7-dbba-52e3dae06828",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "cell_type": "code",
      "source": [
        "# Install the necssary packages\n",
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "!pip install autograd"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting autograd\n",
            "  Downloading https://files.pythonhosted.org/packages/08/7a/1ccee2a929d806ba3dbe632a196ad6a3f1423d6e261ae887e5fef2011420/autograd-1.2.tar.gz\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from autograd) (1.14.6)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from autograd) (0.16.0)\n",
            "Building wheels for collected packages: autograd\n",
            "  Running setup.py bdist_wheel for autograd ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/72/6f/c2/40f130cca2c91f31d354bf72de282922479c09ce0b7853c4c5\n",
            "Successfully built autograd\n",
            "Installing collected packages: autograd\n",
            "Successfully installed autograd-1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qOLmcpS_0MJE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1. VAE with horseshoe priors\n",
        "\n",
        "TODO:\n",
        "* Implement a Neural Network using autograd (NOT pytorch) for compatibility with FactorizedHierarchicalInvGamma\n",
        "* Create a class with similar interface as AE and VAE -> calc_vi_loss function"
      ]
    },
    {
      "metadata": {
        "id": "lDDvGLPR69XB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Util functions\n"
      ]
    },
    {
      "metadata": {
        "id": "VlgTvYFF7BLp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import autograd.numpy.random as npr\n",
        "import autograd.numpy as ag_np\n",
        "from autograd.scipy.misc import logsumexp\n",
        "from autograd.scipy.special import gammaln, psi\n",
        "from autograd import grad\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from autograd.scipy.special import gammaln, psi\n",
        "from copy import copy\n",
        "\n",
        "\n",
        "def diag_gaussian_entropy(log_std, D):\n",
        "    return 0.5 * D * (1.0 + np.log(2 * np.pi)) + np.sum(log_std)\n",
        "\n",
        "\n",
        "def inv_gamma_entropy(a, b):\n",
        "    return np.sum(a + np.log(b) + gammaln(a) - (1 + a) * psi(a))\n",
        "\n",
        "\n",
        "def log_normal_entropy(log_std, mu, D):\n",
        "    return np.sum(log_std + mu + 0.5) + (D / 2) * np.log(2 * np.pi)\n",
        "\n",
        "\n",
        "def make_batches(n_data, batch_size):\n",
        "    return [slice(i, min(i+batch_size, n_data)) for i in range(0, n_data, batch_size)]\n",
        "\n",
        "\n",
        "def bce_loss(x, x_prime):\n",
        "    temp1 = x * ag_np.log(x_prime + 1e-10)\n",
        "    temp2 = (1 - x) * ag_np.log(1 - x_prime + 1e-10)\n",
        "    bce = -ag_np.sum(temp1 + temp2)\n",
        "    return bce\n",
        "\n",
        "\n",
        "def sigmoid(x, derivative=False):\n",
        "    return x*(1-x) if derivative else 1/(1+ag_np.exp(-x))\n",
        "\n",
        "\n",
        "def adam(grad, x, callback=None, num_iters=100, step_size=0.001, b1=0.9, b2=0.999, eps=10**-8, polyak=False):\n",
        "    \"\"\"Adapted from autograd.misc.optimizers\"\"\"\n",
        "    m = np.zeros(len(x))\n",
        "    v = np.zeros(len(x))\n",
        "    for i in range(num_iters):\n",
        "        g = grad(x, i)\n",
        "        if callback: callback(x, i, g, polyak)\n",
        "        m = (1 - b1) * g      + b1 * m  # First  moment estimate.\n",
        "        v = (1 - b2) * (g**2) + b2 * v  # Second moment estimate.\n",
        "        mhat = m / (1 - b1**(i + 1))    # Bias correction.\n",
        "        vhat = v / (1 - b2**(i + 1))\n",
        "        x = x - step_size*mhat/(np.sqrt(vhat) + eps)\n",
        "    return x\n",
        "\n",
        "\n",
        "# Base auto-encoder\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def encode(self, x):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def decode(self, z):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def forward(self, x):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def elbos(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def train_accuracy(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def test_accuracy(self):\n",
        "        raise NotImplementedError"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WtqVJIMR_-28",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Neural network and FactorizedInverseGamma inference"
      ]
    },
    {
      "metadata": {
        "id": "yvI_zErO6vIa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\" Uses a non-centered parameterization of the model.\n",
        "    Fully factorized Gaussian + IGamma Variational distribution\n",
        "\tq = N(w_ijl | m_ijl, sigma^2_ijl) N(ln \\tau_kl | params) IGamma(\\lambda_kl| params)\n",
        "\tIGamma(\\tau_l | params) IGamma(\\lambda_l| params)\n",
        "\"\"\"\n",
        "\n",
        "import autograd.numpy.random as npr\n",
        "# import autograd.numpy as ag_np\n",
        "import autograd.numpy as np\n",
        "from autograd.scipy.misc import logsumexp\n",
        "from autograd.scipy.special import gammaln, psi\n",
        "from autograd import grad\n",
        "from copy import copy\n",
        "# import numpy as np\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "A neural network class built on autograd for compatibility with Hs prior current implementations\n",
        "\"\"\"\n",
        "class NeuralNetworkAutoGrad():\n",
        "    def __init__(self, nn_structure=[32], n_dims_input=1, n_dims_output=1, \\\n",
        "                 weight_fill_func=np.zeros, bias_fill_func=np.zeros, activation_func=lambda x: ag_np.maximum(0, x)):\n",
        "\n",
        "        self.nn_param_list = []\n",
        "        self.activation_func = activation_func\n",
        "        self.n_dims_input = n_dims_input\n",
        "        self.n_dims_output = n_dims_output\n",
        "\n",
        "        n_hiddens_per_layer_list = [n_dims_input] + nn_structure + [n_dims_output]\n",
        "\n",
        "        # Given full network size list is [a, b, c, d, e]\n",
        "        # For loop should loop over (a,b) , (b,c) , (c,d) , (d,e)\n",
        "        for n_in, n_out in zip(n_hiddens_per_layer_list[:-1], n_hiddens_per_layer_list[1:]):\n",
        "            self.nn_param_list.append(\n",
        "                dict(\n",
        "                    w=weight_fill_func((n_in, n_out)),\n",
        "                    b=bias_fill_func((n_out,)),\n",
        "                ))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer_id, layer_dict in enumerate(self.nn_param_list):\n",
        "            if layer_id == 0:\n",
        "                if x.ndim > 1:\n",
        "                    in_arr = x\n",
        "                else:\n",
        "                    if x.size == self.nn_param_list[0]['w'].shape[0]:\n",
        "                        in_arr = x[ag_np.newaxis, :]\n",
        "                    else:\n",
        "                        in_arr = x[:, ag_np.newaxis]\n",
        "            else:\n",
        "                in_arr = self.activation_func(out_arr)\n",
        "            out_arr = ag_np.dot(in_arr, layer_dict['w']) + layer_dict['b']\n",
        "        return ag_np.squeeze(out_arr)\n",
        "\n",
        "\n",
        "class FactorizedHierarchicalInvGamma:\n",
        "    def __init__(self, n_weights, lambda_a, lambda_b, lambda_b_global, tau_a, shapes, train_stats, classification=True,\n",
        "                 n_data=None):\n",
        "        self.name = \"Factorized Hierarchical Inverse Gamma Variational Approximation\"\n",
        "        self.classification = classification\n",
        "        self.n_weights = n_weights\n",
        "        self.shapes = shapes\n",
        "        self.num_hidden_layers = len(shapes) - 1\n",
        "        self.lambda_a_prior = lambda_a\n",
        "        self.lambda_b_prior = lambda_b\n",
        "        self.lambda_a_prior_global = 0.5\n",
        "        self.lambda_b_prior_global = lambda_b_global\n",
        "        self.lambda_a_prior_oplayer = 0.5\n",
        "        self.lambda_b_prior_oplayer = 1.\n",
        "        self.tau_a_prior = tau_a\n",
        "        self.tau_a_prior_global = 0.5\n",
        "        self.tau_a_prior_oplayer = 0.5\n",
        "        self.l2pi = np.log(2 * np.pi)\n",
        "        self.n_data = n_data\n",
        "        self.noise_entropy = None\n",
        "\n",
        "\n",
        "    ######### PACK UNPACK PARAMS #################################################\n",
        "    def initialize_variational_params(self, param_scale=1):\n",
        "        # Initialize weights\n",
        "        wlist = list()\n",
        "        for m, n in self.shapes:\n",
        "            wlist.append(npr.randn(m * n) * np.sqrt(2 / m))\n",
        "            wlist.append(np.zeros(n))  # bias\n",
        "        w = np.concatenate(wlist)\n",
        "        log_sigma = param_scale * npr.randn(w.shape[0]) - 10.\n",
        "        # initialize scale parameters\n",
        "        self.tot_outputs = 0\n",
        "        for _, num_hl_outputs in self.shapes:\n",
        "            self.tot_outputs += num_hl_outputs\n",
        "        # No hs priors on the outputs\n",
        "        self.tot_outputs = self.tot_outputs - self.shapes[-1][1]\n",
        "\n",
        "        tau_mu, tau_log_sigma, tau_global_mu, tau_global_log_sigma, tau_oplayer_mu, tau_oplayer_log_sigma = \\\n",
        "            self.initialize_scale_from_prior()\n",
        "        init_params = np.concatenate([w.ravel(), log_sigma.ravel(),\n",
        "                                          tau_mu.ravel(), tau_log_sigma.ravel(), tau_global_mu.ravel(),\n",
        "                                          tau_global_log_sigma.ravel(), tau_oplayer_mu, tau_oplayer_log_sigma])\n",
        "\n",
        "        return init_params\n",
        "\n",
        "    def initialize_scale_from_prior(self):\n",
        "        # scale parameters (hidden + observed),\n",
        "        self.lambda_a_hat = (self.tau_a_prior + self.lambda_a_prior) * np.ones([self.tot_outputs, 1]).ravel()\n",
        "        self.lambda_b_hat = (1.0 / self.lambda_b_prior ** 2) * np.ones([self.tot_outputs, 1]).ravel()\n",
        "        self.lambda_a_hat_global = (self.tau_a_prior_global + self.lambda_a_prior_global)  \\\n",
        "            * np.ones([self.num_hidden_layers, 1]).ravel()\n",
        "        self.lambda_b_hat_global = (1.0 / self.lambda_b_prior_global ** 2) * np.ones(\n",
        "            [self.num_hidden_layers, 1]).ravel()\n",
        "        # set oplayer lambda param\n",
        "        self.lambda_a_hat_oplayer = np.array(self.tau_a_prior_oplayer + self.lambda_a_prior_oplayer).reshape(-1)\n",
        "        self.lambda_b_hat_oplayer = (1.0 / self.lambda_b_prior_oplayer ** 2) * np.ones([1]).ravel()\n",
        "        # sample from half cauchy and log to initialize the mean of the log normal\n",
        "        sample = np.abs(self.lambda_b_prior * (npr.randn(self.tot_outputs) / npr.randn(self.tot_outputs)))\n",
        "        tau_mu = np.log(sample)\n",
        "        tau_log_sigma = npr.randn(self.tot_outputs) - 10.\n",
        "        # one tau_global for each hidden layer\n",
        "        sample = np.abs(\n",
        "            self.lambda_b_prior_global * (npr.randn(self.num_hidden_layers) / npr.randn(self.num_hidden_layers)))\n",
        "        tau_global_mu = np.log(sample)\n",
        "        tau_global_log_sigma = npr.randn(self.num_hidden_layers) - 10.\n",
        "        # one tau for all op layer weights\n",
        "        sample = np.abs(self.lambda_b_hat_oplayer * (npr.randn() / npr.randn()))\n",
        "        tau_oplayer_mu = np.log(sample)\n",
        "        tau_oplayer_log_sigma = npr.randn(1) - 10.\n",
        "\n",
        "        return tau_mu, tau_log_sigma, tau_global_mu, tau_global_log_sigma, tau_oplayer_mu, tau_oplayer_log_sigma\n",
        "\n",
        "    def unpack_params(self, params):\n",
        "        # unpack params\n",
        "        w_vect = params[:self.n_weights]\n",
        "        num_std = 2 * self.n_weights\n",
        "        \n",
        "        sigma = np.log(1 + np.exp(params[self.n_weights:num_std]))\n",
        "        \n",
        "        tau_mu = params[num_std:num_std + self.tot_outputs]\n",
        "        tau_sigma = np.log(\n",
        "            1 + np.exp(params[num_std + self.tot_outputs:num_std + 2 * self.tot_outputs]))\n",
        "        tau_mu_global = params[num_std + 2 * self.tot_outputs: num_std + 2 * self.tot_outputs + self.num_hidden_layers]\n",
        "        tau_sigma_global = np.log(1 + np.exp(params[num_std + 2 * self.tot_outputs + self.num_hidden_layers:num_std +\n",
        "                                                                    2 * self.tot_outputs + 2 * self.num_hidden_layers]))\n",
        "        tau_mu_oplayer = params[num_std + 2 * self.tot_outputs + 2 * self.num_hidden_layers: num_std +\n",
        "                                                                2 * self.tot_outputs + 2 * self.num_hidden_layers + 1]\n",
        "        tau_sigma_oplayer = np.log(\n",
        "            1 + np.exp(params[num_std + 2 * self.tot_outputs + 2 * self.num_hidden_layers + 1:]))\n",
        "\n",
        "        return w_vect, sigma, tau_mu, tau_sigma, tau_mu_global, tau_sigma_global, tau_mu_oplayer, tau_sigma_oplayer\n",
        "\n",
        "    def unpack_layer_weight_variances(self, sigma_vect):\n",
        "        for m, n in self.shapes:\n",
        "            yield sigma_vect[:m * n].reshape((m, n)), sigma_vect[m * n:m * n + n]\n",
        "            sigma_vect = sigma_vect[(m + 1) * n:]\n",
        "\n",
        "    def unpack_layer_weight_priors(self, tau_vect):\n",
        "        for m, n in self.shapes:\n",
        "            yield tau_vect[:n]\n",
        "            tau_vect = tau_vect[n:]\n",
        "\n",
        "    def unpack_layer_weights(self, w_vect):\n",
        "        for m, n in self.shapes:\n",
        "            yield w_vect[:m * n].reshape((m, n)), w_vect[m * n:m * n + n]\n",
        "            w_vect = w_vect[(m + 1) * n:]\n",
        "\n",
        "    ######### Fixed Point Updates ################################## #####\n",
        "    def fixed_point_updates(self, params):\n",
        "        if self.classification:\n",
        "            w_vect, sigma, tau_mu, tau_sigma, tau_mu_global, tau_sigma_global, tau_mu_oplayer, tau_sigma_oplayer = \\\n",
        "                self.unpack_params(params)\n",
        "        else:\n",
        "            w_vect, sigma, tau_mu, tau_sigma, tau_mu_global, tau_sigma_global, tau_mu_oplayer, tau_sigma_oplayer, _, _ \\\n",
        "                = self.unpack_params(params)\n",
        "        # update lambda moments\n",
        "        self.lambda_b_hat = np.exp(-tau_mu + 0.5 * tau_sigma ** 2) + (1. / self.lambda_b_prior ** 2)\n",
        "        self.lambda_b_hat_global = np.exp(-tau_mu_global + 0.5 * tau_sigma_global ** 2) + (\n",
        "            1. / self.lambda_b_prior_global ** 2)\n",
        "        self.lambda_b_hat_oplayer = np.exp(-tau_mu_oplayer + 0.5 * tau_sigma_oplayer ** 2) + (\n",
        "            1. / self.lambda_b_prior_oplayer ** 2)\n",
        "        return None\n",
        "\n",
        "    ######### ELBO CALC ################################################\n",
        "    def forward(self, mu_vect, sigma_vect, tau_mu_vect, tau_sigma_vect, tau_mu_global, tau_sigma_global,\n",
        "                tau_mu_oplayer, tau_sigma_oplayer, inputs):\n",
        "        for layer_id, (mu, var, tau_mu, tau_sigma) in enumerate(\n",
        "                zip(self.unpack_layer_weights(mu_vect), self.unpack_layer_weight_variances(sigma_vect),\n",
        "                    self.unpack_layer_weight_priors(tau_mu_vect),\n",
        "                    self.unpack_layer_weight_priors(tau_sigma_vect))):\n",
        "            w, b = mu\n",
        "            sigma__w, sigma_b = var\n",
        "            if layer_id < len(self.shapes) - 1:\n",
        "                scale_mu = 0.5 * (tau_mu + tau_mu_global[layer_id])\n",
        "                scale_v = 0.25 * (tau_sigma ** 2 + tau_sigma_global[layer_id] ** 2)\n",
        "                scale = np.exp(scale_mu + np.sqrt(scale_v) * npr.randn(tau_mu.shape[0]))\n",
        "                mu_w = np.dot(inputs, w) + b\n",
        "                v_w = np.dot(inputs ** 2, sigma__w ** 2) + sigma_b ** 2\n",
        "                outputs = (np.sqrt(v_w) / np.sqrt(inputs.shape[1])) * np.random.normal(size=mu_w.shape) + mu_w\n",
        "                outputs = scale * outputs\n",
        "                inputs = outputs * (outputs > 0)\n",
        "            else:\n",
        "                op_scale_mu = 0.5 * tau_mu_oplayer\n",
        "                op_scale_v = 0.25 * tau_sigma_oplayer ** 2\n",
        "                Ekappa_half = np.exp(op_scale_mu + np.sqrt(op_scale_v) * npr.randn())\n",
        "                mu_w = np.dot(inputs, w) + b\n",
        "                v_w = np.dot(inputs ** 2, sigma__w ** 2) + sigma_b ** 2\n",
        "                outputs = Ekappa_half * (np.sqrt(v_w) / np.sqrt(inputs.shape[1])) * np.random.normal(\n",
        "                    size=mu_w.shape) + mu_w\n",
        "        return outputs\n",
        "\n",
        "    def EPw_Gaussian(self, prior_precision, w, sigma):\n",
        "        \"\"\"\"\\int q(z) log p(z) dz, assuming gaussian q(z) and p(z)\"\"\"\n",
        "        wD = w.shape[0]\n",
        "        prior_wvar_ = 1. / prior_precision\n",
        "        a = - 0.5 * wD * np.log(2 * np.pi) - 0.5 * wD * np.log(prior_wvar_) - 0.5 * prior_precision * (\n",
        "            np.dot(w.T, w) + np.sum((sigma ** 2)))\n",
        "        return a\n",
        "\n",
        "    def EP_Gamma(self, Egamma, Elog_gamma):\n",
        "        \"\"\" Enoise precision \"\"\"\n",
        "        return self.noise_a * np.log(self.noise_b) - gammaln(self.noise_a) + (\n",
        "                                                            - self.noise_a - 1) * Elog_gamma - self.noise_b * Egamma\n",
        "\n",
        "    def EPtaulambda(self, tau_mu, tau_sigma, tau_a_prior, lambda_a_prior,\n",
        "                    lambda_b_prior, lambda_a_hat, lambda_b_hat):\n",
        "        \"\"\" E[ln p(\\tau | \\lambda)] + E[ln p(\\lambda)]\"\"\"\n",
        "        etau_given_lambda = -gammaln(tau_a_prior) - tau_a_prior * (np.log(lambda_b_hat) - psi(lambda_a_hat)) + (\n",
        "                            -tau_a_prior - 1.) * tau_mu - np.exp(-tau_mu + 0.5 * tau_sigma ** 2) * (lambda_a_hat /\n",
        "                                               lambda_b_hat)\n",
        "        elambda = -gammaln(lambda_a_prior) - 2 * lambda_a_prior * np.log(lambda_b_prior) + (-lambda_a_prior - 1.) * (\n",
        "            np.log(lambda_b_hat) - psi(lambda_a_hat)) - (1. / lambda_b_prior ** 2) * (lambda_a_hat / lambda_b_hat)\n",
        "        return np.sum(etau_given_lambda) + np.sum(elambda)\n",
        "\n",
        "    def entropy(self, sigma, tau_sigma, tau_mu, tau_sigma_global, tau_mu_global, tau_sigma_oplayer, tau_mu_oplayer):\n",
        "        ent_w = diag_gaussian_entropy(np.log(sigma), self.n_weights)\n",
        "        ent_tau = log_normal_entropy(np.log(tau_sigma), tau_mu, self.tot_outputs) + log_normal_entropy(\n",
        "            np.log(tau_sigma_global), tau_mu_global, self.num_hidden_layers) + log_normal_entropy(\n",
        "            np.log(tau_sigma_oplayer), tau_mu_oplayer, 1)\n",
        "        ent_lambda = inv_gamma_entropy(self.lambda_a_hat, self.lambda_b_hat) + inv_gamma_entropy(\n",
        "            self.lambda_a_hat_global, self.lambda_b_hat_global) + inv_gamma_entropy(self.lambda_a_hat_oplayer,\n",
        "                                                                                    self.lambda_b_hat_oplayer)\n",
        "        return ent_w, ent_tau, ent_lambda\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1hvsSyof7EQh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## HS_VAE implementations"
      ]
    },
    {
      "metadata": {
        "id": "1cydvYRB0OqL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class HS_VAE(VAE):\n",
        "    def __init__(\n",
        "            self,\n",
        "            q_sigma=0.2,\n",
        "            n_dims_code=16,\n",
        "            n_dims_data=64,\n",
        "            hidden_layer_sizes=[32],\n",
        "            classification=True,\n",
        "            batch_size=128,\n",
        "            lambda_b_global=1.0,\n",
        "            warm_up=False,\n",
        "            polyak=False,):\n",
        "\n",
        "        super(HS_VAE, self).__init__()\n",
        "        layer_sizes = (\n",
        "            [n_dims_data] + hidden_layer_sizes + [n_dims_code]\n",
        "        )\n",
        "        self.n_dims_code = n_dims_code\n",
        "        self.q_sigma = q_sigma\n",
        "        self.n_dims_data = n_dims_data\n",
        "\n",
        "        self.shapes = list(zip(layer_sizes[:-1], layer_sizes[1:]))\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.lambda_b_global = lambda_b_global\n",
        "        self.N_weights = sum((m + 1) * n for m, n in self.shapes)\n",
        "        \n",
        "        self.elbo = list()\n",
        "        self.val_ll = list()\n",
        "        self.val_err = list()\n",
        "        self.train_err = list()\n",
        "        self.test_err  = list()\n",
        "        \n",
        "        self.variational_params = None\n",
        "        self.init_params = None\n",
        "        self.polyak_params = None\n",
        "        self.polyak = polyak\n",
        "        self.variational_params_store = {}\n",
        "        self.optimal_elbo_params = None\n",
        "        self.warm_up = warm_up  # if True, anneal in KL\n",
        "\n",
        "        self.M = 100 # TODO: the total number of mini-batches (Number of nodes?)\n",
        "\n",
        "        # TODO: in Ghosh's implementation, mu is the mean of all x_train and\n",
        "        # TODO: sigma is the standard deviation of all x_train\n",
        "        train_stats = dict()\n",
        "        train_stats['mu'] = 0\n",
        "        train_stats['sigma'] = 1\n",
        "\n",
        "        # Initialize the encoder and decoder\n",
        "        # TODO: incorporate Factorized Hierarchical Inverse Gamma\n",
        "        self.horseshoe_encoder = FactorizedHierarchicalInvGamma(\n",
        "            lambda_a=0.5, lambda_b=1.0,\n",
        "            lambda_b_global=self.lambda_b_global, tau_a=0.5,\n",
        "            shapes=self.shapes, train_stats=train_stats,\n",
        "            classification=classification,\n",
        "            n_weights=self.N_weights)\n",
        "\n",
        "        self.decoder = NeuralNetworkAutoGrad(nn_structure=hidden_layer_sizes, \\\n",
        "                                             n_dims_input=n_dims_code, n_dims_output=n_dims_data)\n",
        "\n",
        "    def neg_elbo(self, params, epoch, xs_ND, ys_ND, matrix_entries):\n",
        "        \"\"\"\n",
        "\n",
        "        :param params:\n",
        "        :param epoch:\n",
        "        :param xs_ND:\n",
        "        :param ys_ND:\n",
        "        :param matrix_entries:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if self.warm_up:\n",
        "            nt = 200  # linear increments between 0 and 1 up to nt (1 after nt)\n",
        "            temperature = epoch / nt\n",
        "            if temperature > 1:\n",
        "                temperature = 1\n",
        "        else:\n",
        "            temperature = 1\n",
        "\n",
        "        # Unpack the current parameters using function provided by Inv-Gamma\n",
        "        w_vect, sigma, tau_mu, tau_sigma, tau_mu_global, tau_sigma_global, tau_mu_oplayer, tau_sigma_oplayer \\\n",
        "                = self.horseshoe_encoder.unpack_params(params)\n",
        "\n",
        "        # Compute the x_NC codes and y_NC codes (latent embeddings)\n",
        "        xs_NC = self.horseshoe_encoder.forward(w_vect, sigma, tau_mu, tau_sigma,\\\n",
        "                                       tau_mu_global, tau_sigma_global, tau_mu_oplayer, tau_sigma_oplayer, xs_ND)\n",
        "\n",
        "        ys_NC = self.horseshoe_encoder.forward(w_vect, sigma, tau_mu, tau_sigma,\\\n",
        "                                       tau_mu_global, tau_sigma_global, tau_mu_oplayer, tau_sigma_oplayer, ys_ND)\n",
        "\n",
        "\n",
        "        # Compute log likelihood\n",
        "        # TODO: check if there is any scaling factor I need to compute based on implementions in InvGamma\n",
        "        log_lik = self.log_likelihood_compute(xs_NC, ys_NC, xs_ND, ys_ND, matrix_entries)\n",
        "\n",
        "        # Compute the entropies and log_prior\n",
        "        # Note that this is taken from implementations from inv-Gamma\n",
        "        log_prior, ent_w, ent_tau, ent_lambda = self.entropy_compute(params)\n",
        "\n",
        "        log_variational = ent_w + ent_tau + ent_lambda\n",
        "        minibatch_rescaling = 1. / self.M\n",
        "        ELBO = temperature * minibatch_rescaling * (log_variational + log_prior) + log_lik\n",
        "        return -1 * ELBO\n",
        "\n",
        "    def entropy_compute(self, params):\n",
        "        w_vect, sigma, tau_mu, tau_sigma, tau_mu_global, tau_sigma_global, tau_mu_oplayer, tau_sigma_oplayer \\\n",
        "                = self.horseshoe_encoder.unpack_params(params)\n",
        "\n",
        "        log_prior = self.horseshoe_encoder.EPw_Gaussian(1., w_vect, sigma)\n",
        "        log_prior = log_prior + \\\n",
        "                    self.horseshoe_encoder.EPtaulambda(tau_mu, tau_sigma, self.horseshoe_encoder.tau_a_prior, \\\n",
        "                                                       self.horseshoe_encoder.lambda_a_prior, self.horseshoe_encoder.lambda_b_prior,\n",
        "                                                       self.horseshoe_encoder.lambda_a_hat, self.horseshoe_encoder.lambda_b_hat) + \\\n",
        "                    self.horseshoe_encoder.EPtaulambda(tau_mu_global, tau_sigma_global, self.horseshoe_encoder.tau_a_prior_global,\n",
        "                                     self.horseshoe_encoder.lambda_a_prior_global, self.horseshoe_encoder.lambda_b_prior_global, self.horseshoe_encoder.lambda_a_hat_global,\n",
        "                                     self.horseshoe_encoder.lambda_b_hat_global) + \\\n",
        "                    self.horseshoe_encoder.EPtaulambda(tau_mu_oplayer, tau_sigma_oplayer, self.horseshoe_encoder.tau_a_prior_oplayer,\n",
        "                                     self.horseshoe_encoder.lambda_a_prior_oplayer, self.horseshoe_encoder.lambda_b_prior_oplayer,\n",
        "                                     self.horseshoe_encoder.lambda_a_hat_oplayer, self.horseshoe_encoder.lambda_b_hat_oplayer)\n",
        "\n",
        "        # Compute the entropies\n",
        "        ent_w, ent_tau, ent_lambda = self.horseshoe_encoder.entropy(sigma, tau_sigma, tau_mu, tau_sigma_global, tau_mu_global,\n",
        "                                                  tau_sigma_oplayer, tau_mu_oplayer)\n",
        "\n",
        "        return log_prior, ent_w, ent_tau, ent_lambda\n",
        "\n",
        "    def log_likelihood_compute(self, xs_NC, ys_NC, xs_ND, ys_NDs, matrix_entries):\n",
        "        \"\"\"\n",
        "        :param xs_NC:\n",
        "        :param ys_NC:\n",
        "        :param xs_ND:\n",
        "        :param ys_NDs:\n",
        "        :param matrix_entries:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        n_mc_samples = 20\n",
        "        log_lik      = 0.\n",
        "\n",
        "        # Generate samples from N(mx_NC, q_sigma) to compute the following\n",
        "        # E_q[log p(x_ND|mx_NC)]\n",
        "        for ss in range(n_mc_samples):\n",
        "            sample_xz_NC = self.draw_sample_from_q(xs_NC)\n",
        "            sample_xproba_ND = self.decode(sample_xz_NC)\n",
        "\n",
        "            sample_yz_NC = self.draw_sample_from_q(ys_NC)\n",
        "            sample_yproba_ND = self.decode(sample_yz_NC)\n",
        "\n",
        "            # Use MSE to measure reconstruction loss\n",
        "            # Since MSE is equivalent to log gaussian loss\n",
        "            log_ll_x_reconstructed = -0.5 * ag_np.sum((sample_xproba_ND - xs_ND)**2)\n",
        "            log_ll_y_reconstructed = -0.5 * ag_np.sum((sample_yproba_ND - ys_NDs)**2)\n",
        "\n",
        "            # KL divergence from q(mu, sigma) to prior (std normal)\n",
        "            log_lik += 1 / n_mc_samples * (log_ll_x_reconstructed + log_ll_y_reconstructed)\n",
        "\n",
        "        # Compute the loss from adjacency matrix reconstruction\n",
        "        # Get number of entries\n",
        "        num_samples = len(matrix_entries)\n",
        "        f_predict = ag_np.zeros(num_samples)\n",
        "\n",
        "        # Compute\n",
        "        # E_q[log p(A_ij|x_i, x_j)] = E_q[Bern(A_ij|sigmoid(x_i dot x_j))]\n",
        "        for ss in range(n_mc_samples):\n",
        "            # These two should have the same shape\n",
        "            # which is (N*C)\n",
        "            sample_xz_NC = self.draw_sample_from_q(xs_NC)\n",
        "            sample_yz_NC = self.draw_sample_from_q(ys_NC)\n",
        "\n",
        "            # inner_prod.shape = (N,)\n",
        "            inner_prod = ag_np.sum(sample_xz_NC * sample_yz_NC, axis=1)\n",
        "            f_predict += 1 / n_mc_samples * sigmoid(inner_prod)\n",
        "\n",
        "        matrix_reconstruction_loss = bce_loss(matrix_entries, f_predict)\n",
        "        log_lik_matrix_reconstruction = -matrix_reconstruction_loss\n",
        "        log_lik += log_lik_matrix_reconstruction\n",
        "        return log_lik\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder.forward(z)\n",
        "\n",
        "    def draw_sample_from_q(self, xs_NC):\n",
        "        N = xs_NC.shape[0]\n",
        "        # The dimension of the code\n",
        "        C = self.n_dims_code\n",
        "        # Draw standard normal samples \"epsilon\"\n",
        "        # Use the reparameterization trick\n",
        "        eps_NC = np.random.randn(N, C)\n",
        "        z_NC = xs_NC + eps_NC * self.q_sigma\n",
        "        return z_NC\n",
        "\n",
        "    def calc_vi_loss(self, params, t):\n",
        "        idx = t % self.M\n",
        "\n",
        "        # TODO: do mini-batch computation here\n",
        "        x_ND = self.feature_vectors[idx]\n",
        "        observed_entries = self.train_adjacency_matrix[idx]\n",
        "        other_vec_idx = [entry[0][1] for entry in observed_entries]\n",
        "        matrix_entries = np.asarray([entry[1] for entry in observed_entries])\n",
        "\n",
        "        xs_ND = []\n",
        "        ys_ND = []\n",
        "        for idy in other_vec_idx:\n",
        "            xs_ND.append(x_ND)\n",
        "            ys_ND.append(self.feature_vectors[idy, :])\n",
        "\n",
        "        xs_ND = ag_np.asarray(xs_ND)\n",
        "        ys_ND = ag_np.asarray(ys_ND)\n",
        "\n",
        "        return self.neg_elbo(params, t / self.M, xs_ND, ys_ND, matrix_entries)\n",
        "\n",
        "\n",
        "    def fit(self, feature_vectors, train_adjacency_matrix, n_epochs=10, l_rate=0.01, test_adjacency_matrix=None):\n",
        "        self.M = feature_vectors.shape[0]\n",
        "        # TODO: Number of nodes is the number of minibatches in an epoch?\n",
        "        self.feature_vectors = feature_vectors\n",
        "        self.train_adjacency_matrix = train_adjacency_matrix\n",
        "\n",
        "        # TODO: look into how to do mini-batches\n",
        "        self.test_adjacency_matrix = test_adjacency_matrix\n",
        "\n",
        "        def callback(params, t, g, decay=0.999):\n",
        "            if self.polyak:\n",
        "                # exponential moving average.\n",
        "                self.polyak_params = decay * self.polyak_params + (1 - decay) * params\n",
        "\n",
        "            elbo = -self.calc_vi_loss(params, t)\n",
        "            self.elbo.append(elbo)\n",
        "\n",
        "            if (t % self.M) == 0:\n",
        "                train_err = self.compute_accuracy(params, test=False)\n",
        "                test_err  = self.compute_accuracy(params, test=True)\n",
        "                self.train_err.append(train_err)\n",
        "                self.test_err.append(test_err)\n",
        "\n",
        "                if ((t / self.M) % 10) == 0:\n",
        "                    print(\"Epoch {} elbo {} train-accuracy {} test-accuracy {}\".format(t / self.M, self.elbo[-1],\n",
        "                                                                                         self.train_err[-1], self.test_err[-1]))\n",
        "\n",
        "            if (t % 250) == 0:\n",
        "                # store optimization progress.\n",
        "                self.variational_params_store[t] = copy(params)\n",
        "            if t > 2:\n",
        "                if self.elbo[-1] > max(self.elbo[:-1]):\n",
        "                    self.optimal_elbo_params = copy(params)\n",
        "\n",
        "            # update inverse gamma distributions\n",
        "            self.horseshoe_encoder.fixed_point_updates(params)\n",
        "\n",
        "        # Variational parameters include tau, lambda, v, Vu etc\n",
        "        init_var_params = self.horseshoe_encoder.initialize_variational_params()\n",
        "        self.init_params = copy(init_var_params)\n",
        "\n",
        "        if self.polyak:\n",
        "            self.polyak_params = copy(init_var_params)\n",
        "\n",
        "        gradient  = grad(self.calc_vi_loss, 0)\n",
        "        num_iters = n_epochs * self.M  # one iteration = one set of param updates\n",
        "\n",
        "        # Run the algorithm using Adam with callback\n",
        "        self.variational_params = adam(gradient, init_var_params,\n",
        "                                       step_size=l_rate, num_iters=num_iters, callback=callback,\n",
        "                                       polyak=self.polyak)\n",
        "\n",
        "    def compute_accuracy(self, params, test=True):\n",
        "        W_vect, sigma, tau_mu, tau_sigma, tau_mu_global, tau_sigma_global, tau_mu_oplayer, tau_sigma_oplayer = \\\n",
        "            self.horseshoe_encoder.unpack_params(params)\n",
        "\n",
        "        z_NC = self.horseshoe_encoder.forward(W_vect, sigma, tau_mu, tau_sigma, tau_mu_global, tau_sigma_global,\n",
        "                             tau_mu_oplayer, tau_sigma_oplayer, self.feature_vectors)\n",
        "\n",
        "        # z_NC has shape (N,C)\n",
        "\n",
        "        num_accurate = 0.0\n",
        "        num_observed = 0.0\n",
        "\n",
        "        latent_adj_mat = np.dot(z_NC, z_NC.T)\n",
        "\n",
        "        if test:\n",
        "            adjancency_matrix = self.test_adjacency_matrix\n",
        "        else:\n",
        "            adjancency_matrix = self.train_adjacency_matrix\n",
        "\n",
        "        for row in adjancency_matrix:\n",
        "            for coor, val in row:\n",
        "                if (latent_adj_mat[coor[0]][coor[1]] < 0.0 and val == 0.0 or\n",
        "                                latent_adj_mat[coor[0]][coor[1]] >= 0.0 and val == 1.0):\n",
        "                    num_accurate += 1\n",
        "                num_observed += 1\n",
        "\n",
        "        return num_accurate / num_observed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IdhSSbsHzrsB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2. Standard VAE class\n",
        "TODO\n",
        "* (Not important) AutoEncoder implementation (not Variational) for comparisons"
      ]
    },
    {
      "metadata": {
        "id": "sYNQvpF8AQYP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            n_dims_code=16,\n",
        "            n_dims_data=32,\n",
        "            hidden_layer_sizes=[32],\n",
        "            encoder=True,):\n",
        "        \n",
        "        \"\"\"\n",
        "        q_sigma = 0.2\n",
        "        \"\"\"\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "\n",
        "        self.n_dims_data = n_dims_data\n",
        "        self.n_dims_code = n_dims_code\n",
        "        layer_sizes = (\n",
        "            [n_dims_data] + hidden_layer_sizes + [n_dims_code]\n",
        "        )\n",
        "        self.n_layers = len(layer_sizes) - 1\n",
        "\n",
        "        if not encoder:\n",
        "            layer_sizes = [a for a in reversed(layer_sizes)]\n",
        "\n",
        "        self.activations = list()\n",
        "        self.params = nn.ModuleList()\n",
        "        for (n_in, n_out) in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
        "            self.params.append(nn.Linear(n_in, n_out))\n",
        "            self.activations.append(F.relu)\n",
        "        self.activations[-1] = lambda a: a\n",
        "            \n",
        "    def forward(self, x):\n",
        "        # Note that if x contains multiple instance\n",
        "        # if x.shape = (num_sample, in_dim)\n",
        "        # then the output shape will be (num_sample, out_dim)\n",
        "        cur_arr = x\n",
        "        for ll in range(self.n_layers):\n",
        "            linear_func = self.params[ll]\n",
        "            a_func = self.activations[ll]\n",
        "            cur_arr = a_func(linear_func(cur_arr))\n",
        "        mu_NC = cur_arr\n",
        "        return mu_NC\n",
        "      \n",
        "      \n",
        "class VariationalAutoencoder(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            q_sigma=0.2,\n",
        "            n_dims_code=16,\n",
        "            n_dims_data=64,\n",
        "            hidden_layer_sizes=[32],\n",
        "    ):\n",
        "\n",
        "        super(VariationalAutoencoder, self).__init__()\n",
        "        self.n_dims_data = n_dims_data\n",
        "        self.n_dims_code = n_dims_code\n",
        "        self.q_sigma = torch.Tensor([float(q_sigma)])\n",
        "        \n",
        "        # Encoder network\n",
        "        self.encoder = NeuralNetwork(\n",
        "            n_dims_code=n_dims_code,\n",
        "            n_dims_data=n_dims_data,\n",
        "            hidden_layer_sizes=hidden_layer_sizes,\n",
        "            encoder=True,\n",
        "        )\n",
        "        # Decoder network\n",
        "        self.decoder = NeuralNetwork(\n",
        "            n_dims_code=n_dims_code,\n",
        "            n_dims_data=n_dims_data,\n",
        "            hidden_layer_sizes=hidden_layer_sizes,\n",
        "            encoder=False,\n",
        "        )\n",
        "        self.train_losses = []\n",
        "        self.test_losses = []\n",
        "        self.ELBOs = []\n",
        "        self.epochs = []\n",
        "        self.all_epochs = []\n",
        "        \n",
        "        \n",
        "    def elbo_epochs(self):\n",
        "        return self.all_epochs\n",
        "      \n",
        "    def loss_epochs(self):\n",
        "        return self.epochs\n",
        "        \n",
        "    def elbos(self):\n",
        "        return self.ELBOs\n",
        "        \n",
        "    def tr_losses(self):\n",
        "        return self.train_losses \n",
        "        \n",
        "    def te_losses(self):\n",
        "        return self.test_losses\n",
        "\n",
        "    def forward(self, x_ND):\n",
        "        \"\"\"\n",
        "        Run entire probabilistic autoencoder on input (encode then decode)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        xproba_ND : 1D array, size of x_ND\n",
        "        \"\"\"\n",
        "        mu_NC = self.encode(x_ND)\n",
        "        z_NC = self.draw_sample_from_q(mu_NC)\n",
        "        return self.decode(z_NC), mu_NC\n",
        "\n",
        "    \n",
        "    def draw_sample_from_q(self, mu_NC):\n",
        "        ''' Draw sample from the probabilistic encoder q(z|mu(x), \\sigma)\n",
        "\n",
        "        We assume that \"q\" is Normal with:\n",
        "        * mean mu (argument of this function)\n",
        "        * stddev q_sigma (attribute of this class, use self.q_sigma)\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "        mu_NC : tensor-like, N x C\n",
        "            Mean of the encoding for each of the N images in minibatch.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        z_NC : tensor-like, N x C\n",
        "            Exactly one sample vector for each of the N images in minibatch.\n",
        "        '''\n",
        "        # Number of samples\n",
        "        N = mu_NC.shape[0]\n",
        "        \n",
        "        # The dimension of the code\n",
        "        C = self.n_dims_code\n",
        "\n",
        "        if self.training:\n",
        "            # Draw standard normal samples \"epsilon\"\n",
        "            # Use the reparameterization trick\n",
        "            eps_NC = torch.randn(N, C)\n",
        "            z_NC = mu_NC + eps_NC * self.q_sigma\n",
        "            return z_NC\n",
        "        else:\n",
        "            # For evaluations, we always just use the mean\n",
        "            return mu_NC\n",
        "\n",
        "    def encode(self, x_ND):\n",
        "        \"\"\"\n",
        "        Args\n",
        "        ----\n",
        "        x_ND: the observation vector\n",
        "        \"\"\"\n",
        "        return self.encoder.forward(x_ND)\n",
        "\n",
        "    def decode(self, z_NC):\n",
        "        \"\"\"\n",
        "        Args\n",
        "        ----\n",
        "        z_NC: the code vector\n",
        "        \"\"\"\n",
        "        return self.decoder.forward(z_NC)\n",
        "\n",
        "    def binary_predict_error_rate(self, f_predict, f_true):\n",
        "        \"\"\"\n",
        "        \n",
        "        \"\"\"\n",
        "        length = f_predict.size()[0]\n",
        "        f_predict_binary = (f_predict > 0.5).type(torch.FloatTensor)\n",
        "        error = torch.sum((f_predict_binary - f_true).abs_()) / length\n",
        "        return error\n",
        "\n",
        "    def calc_vi_loss(self, xs_ND, ys_ND, vals, n_mc_samples=1):\n",
        "        \"\"\"\n",
        "        Args\n",
        "\n",
        "        xs_ND: the input feature vectors\n",
        "        ys_ND: the other feature vectors in mini-batch\n",
        "        vals: the entry associated with (x,y)\n",
        "        n_mc_samples: \n",
        "\n",
        "        ----\n",
        "        Returns:\n",
        "        loss\n",
        "\n",
        "        \"\"\"\n",
        "        neg_expected_ll = 0.0\n",
        "        # Given a (potentially) a tensor of observation vectors, \n",
        "        # Encode it into latent space\n",
        "        mx_NC = self.encode(xs_ND)\n",
        "        my_NC = self.encode(ys_ND)\n",
        "        \n",
        "        # Compute the KL divergence\n",
        "        # KL(N(mx_NC, q_sigma) || N(0, I))\n",
        "        kl_xz_NC = -0.5 * torch.sum(1 + torch.log(self.q_sigma ** 2) - mx_NC ** 2 - self.q_sigma ** 2)\n",
        "        kl_yz_NC = -0.5 * torch.sum(1 + torch.log(self.q_sigma ** 2) - my_NC ** 2 - self.q_sigma ** 2)\n",
        "        kl       = kl_xz_NC + kl_yz_NC # Total KL term\n",
        "        \n",
        "        # Generate samples from N(mx_NC, q_sigma) to compute the following\n",
        "        # E_q[log p(x_ND|mx_NC)]\n",
        "        for ss in range(n_mc_samples):\n",
        "            sample_z_NC      = self.draw_sample_from_q(mx_NC)\n",
        "            sample_xproba_ND = self.decode(sample_z_NC)\n",
        "\n",
        "            # Use MSE to measure reconstruction loss\n",
        "            # Since MSE is equivalent to log gaussian loss\n",
        "            sample_mse_loss  = F.mse_loss(sample_xproba_ND, xs_ND)\n",
        "\n",
        "            # KL divergence from q(mu, sigma) to prior (std normal)\n",
        "            neg_expected_ll += 1/n_mc_samples * sample_mse_loss\n",
        "        \n",
        "        \n",
        "        # Compute the loss from adjacency matrix reconstruction\n",
        "        # Get number of entries\n",
        "        num_samples = len(vals)\n",
        "        f_predict   = torch.zeros(num_samples)\n",
        "\n",
        "        # Compute\n",
        "        # E_q[log p(A_ij|x_i, x_j)] = E_q[Bern(A_ij|sigmoid(x_i dot x_j))]\n",
        "        \n",
        "        for ss in range(n_mc_samples):\n",
        "            # These two should have the same shape\n",
        "            # which is (N*C)\n",
        "            sample_z_NC = self.draw_sample_from_q(mx_NC)\n",
        "            sample_y_NC = self.draw_sample_from_q(my_NC)\n",
        "\n",
        "            # inner_prod.shape = (N,)\n",
        "            inner_prod  = torch.sum(sample_z_NC * sample_y_NC, dim=1)\n",
        "            f_predict  += 1/n_mc_samples * torch.sigmoid(inner_prod)\n",
        "\n",
        "        # Use binary cross entry loss, NOTE that this is for\n",
        "        # adjacency matrix whose entry values are 0 and 1\n",
        "        # This will need to change for other types of adjacency matrix value\n",
        "        # Use the binary prediction loss with logits\n",
        "        matrix_reconstruction_loss = \\\n",
        "            F.binary_cross_entropy(f_predict, Variable(torch.FloatTensor(vals)), reduction='sum')\n",
        "        \n",
        "        neg_expected_ll += matrix_reconstruction_loss\n",
        "        \n",
        "        return neg_expected_ll, kl, matrix_reconstruction_loss, sample_xproba_ND\n",
        "      \n",
        "    def fit(self, feature_vectors, train_adjacency_matrix, n_epochs=10, test_adjacency_matrix=None):\n",
        "        # Initialize optimizer\n",
        "        optimizer = torch.optim.SGD(self.parameters(), lr=0.01)\n",
        "\n",
        "        for epoch in range(n_epochs): \n",
        "            # Do round-robin optimization\n",
        "            for idx in range(num_nodes):\n",
        "                x_ND = Variable(torch.FloatTensor([feature_vectors[idx, :]]))\n",
        "                ys_ND = list()\n",
        "                observed_entries = train_adjacency_matrix[idx]\n",
        "                other_vec_idx = [entry[0][1] for entry in observed_entries]\n",
        "                vals = [entry[1] for entry in observed_entries]\n",
        "\n",
        "                for idy in other_vec_idx:\n",
        "                    ys_ND.append(feature_vectors[idy, :])\n",
        "\n",
        "                ys_ND = Variable(torch.FloatTensor(ys_ND))\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # NOTE:\n",
        "                # expected_ll refers to the expected log likelihood term of ELBO\n",
        "                # kl refers to the KL divergence term of the ELBO\n",
        "                # matrix_loss refers to the matrix reconstruction loss\n",
        "                neg_expected_ll, KL, matrix_loss, _ = self.calc_vi_loss(x_ND, ys_ND, vals, n_mc_samples=10)\n",
        "\n",
        "                KL = 1/len(observed_entries) * KL\n",
        "                # TODO: scale the KL term\n",
        "                # ELBO loss = negative expected log likelihood + KL\n",
        "                elbo_loss = neg_expected_ll + KL\n",
        "\n",
        "                elbo_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                self.all_epochs.append(epoch)\n",
        "                self.ELBOs.append(-elbo_loss)\n",
        "\n",
        "\n",
        "            if epoch in [0, 1, 25] or epoch % 50 == 0:\n",
        "                all_vectors = Variable(torch.FloatTensor(feature_vectors))\n",
        "                train_accuracy = classification_accuracy(self, all_vectors, train_adjacency_matrix)\n",
        "                test_accuracy = classification_accuracy(self, all_vectors, test_adjacency_matrix)\n",
        "                self.epochs.append(epoch)\n",
        "                self.train_losses.append(1.0 - train_accuracy)\n",
        "                self.test_losses.append(1.0 - test_accuracy)\n",
        "\n",
        "                print(\"epoch: \", epoch, \" - objective loss: \", np.around(elbo_loss.data.item(),4), \" - train accuracy: \",\n",
        "                      np.around(train_accuracy,4), \" - test accuracy: \", np.around(test_accuracy, 4))\n",
        "\n",
        "      \n",
        "      \n",
        "\n",
        "class VariationalAutoencoderHSP(VAE):\n",
        "    def __init__(\n",
        "            self,\n",
        "            q_sigma=0.2,\n",
        "            n_dims_code=16,\n",
        "            n_dims_data=64,\n",
        "            hidden_layer_sizes=[32],\n",
        "            classification=False,\n",
        "            batch_size=128,\n",
        "            lambda_b_global=1.0,\n",
        "            warm_up=False,\n",
        "            polyak=False,\n",
        "    ):\n",
        "\n",
        "        super(VariationalAutoencoderHSP, self).__init__()\n",
        "        layer_sizes = (\n",
        "            [n_dims_data] + hidden_layer_sizes + [n_dims_code]\n",
        "        )\n",
        "        self.shapes = list(zip(layer_sizes[:-1], layer_sizes[1:]))\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.lambda_b_global = lambda_b_global\n",
        "        self.N_weights = sum((m+1)*n for m, n in self.shapes)\n",
        "        self.elbo = list()\n",
        "        self.val_ll = list()\n",
        "        self.val_err = list()\n",
        "        self.train_err = list()\n",
        "        self.variational_params = None\n",
        "        self.init_params = None\n",
        "        self.polyak_params = None\n",
        "        self.polyak = polyak\n",
        "        self.variational_params_store = {}\n",
        "        self.optimal_elbo_params = None\n",
        "        self.warm_up = warm_up  # if True, anneal in KL\n",
        "        # TODO: FactorizedInv-Gamma\n",
        "        # Use Ghosh's implementation of the factorizedInv-Gamma inference\n",
        "        # engine or reimplement his code with pytorch\n",
        "        self.inference_engine = FactorizedHierarchicalInvGamma(\n",
        "            lambda_a=0.5, lambda_b=1.0,\n",
        "            lambda_b_global=self.lambda_b_global, tau_a=0.5,\n",
        "            shapes=self.shapes, train_stats=train_stats,\n",
        "            classification=classification, n_data=self.X.shape[0],\n",
        "            n_weights=self.N_weights)\n",
        "        \n",
        "    def neg_elbo(self, params, epoch, x, y):\n",
        "        if self.warm_up:\n",
        "            nt = 200  # linear increments between 0 and 1 up to nt (1 after nt)\n",
        "            temperature = epoch/nt\n",
        "            if temperature > 1:\n",
        "                temperature = 1\n",
        "        else:\n",
        "            temperature = 1\n",
        "        log_lik, log_prior, ent_w, ent_tau, ent_lam = self.inference_engine.compute_elbo_contribs(params, x, y)\n",
        "        log_variational = ent_w + ent_tau + ent_lam\n",
        "        minibatch_rescaling = 1./self.M\n",
        "        ELBO = temperature * minibatch_rescaling * (log_variational + log_prior) + log_lik\n",
        "        return -1*ELBO\n",
        "      \n",
        "    def variational_objective(self, params, t):\n",
        "        idx = self.batches[t % self.M]\n",
        "        return self.neg_elbo(params, t/self.M, self.X[idx], self.y[idx])\n",
        "\n",
        "    def compute_optimal_test_ll(self, num_samples=100):\n",
        "        if not self.polyak:\n",
        "            return self.inference_engine.compute_test_ll(self.variational_params, self.X_test,\n",
        "                                                         self.y_test, num_samples=num_samples)\n",
        "        else:\n",
        "            return self.inference_engine.compute_test_ll(self.polyak_params, self.X_test,\n",
        "                                                         self.y_test, num_samples=num_samples)\n",
        "          \n",
        "    def fit(self, X, train_adjacency_matrix, n_epochs=10, l_rate=0.01, test_adjacency_matrix=None):\n",
        "        self.X = X\n",
        "        self.Y = train_adjacency_matrix\n",
        "        self.X_test = X\n",
        "        self.Y_test = test_adjacency_matrix\n",
        "        def callback(params, t, g, decay=0.999):\n",
        "            if self.polyak:\n",
        "                # exponential moving average.\n",
        "                self.polyak_params = decay * self.polyak_params + (1 - decay) * params\n",
        "            score = -self.variational_objective(params, t)\n",
        "            self.elbo.append(score)\n",
        "            if (t % self.M) == 0:\n",
        "                if self.polyak:\n",
        "                    val_ll, val_err = self.inference_engine.compute_test_ll(self.polyak_params, self.X_test, self.y_test)\n",
        "                else:\n",
        "                    val_ll, val_err = self.inference_engine.compute_test_ll(params, self.X_test, self.y_test)\n",
        "                train_err = self.inference_engine.compute_train_err(params, self.X, self.y)\n",
        "                self.val_ll.append(val_ll)\n",
        "                self.val_err.append(val_err)\n",
        "                self.train_err.append(train_err)\n",
        "                if ((t / self.M) % 10) == 0:\n",
        "                    if self.inference_engine.classification:\n",
        "                        print(\"Epoch {} lower bound {} train_err {} test_err {} \".format(t/self.M, self.elbo[-1],\n",
        "                                                                                         train_err,\n",
        "                                                                                         self.val_err[-1]))\n",
        "                    else:\n",
        "                        print(\"Epoch {} lower bound {} train_rmse {} test_rmse {} \".format(t / self.M, self.elbo[-1],\n",
        "                                                                                         train_err, self.val_err[-1]))\n",
        "                # randomly permute batch ordering every epoch\n",
        "                self.batches = np.random.permutation(self.batches)\n",
        "            if (t % 250) == 0:\n",
        "                # store optimization progress.\n",
        "                self.variational_params_store[t] = copy(params)\n",
        "            if t > 2:\n",
        "                if self.elbo[-1] > max(self.elbo[:-1]):\n",
        "                    self.optimal_elbo_params = copy(params)\n",
        "            # update inverse gamma distributions\n",
        "            self.inference_engine.fixed_point_updates(params)\n",
        "\n",
        "        init_var_params = self.inference_engine.initialize_variational_params()\n",
        "        self.init_params = copy(init_var_params)\n",
        "        if self.polyak:\n",
        "            self.polyak_params = copy(init_var_params)\n",
        "        gradient = grad(self.variational_objective, 0)\n",
        "        num_iters = n_epochs * model.M  # one iteration = one set of param updates\n",
        "        self.variational_params = adam(gradient, init_var_params,\n",
        "                                       step_size=l_rate, num_iters=num_iters, callback=callback,\n",
        "                                       polyak=model.polyak)\n",
        "\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E72JYLr6TbU8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3. Data processing\n",
        "\n",
        "\n",
        "Synthetic data (and real life data to test on) should include\n",
        "* Adjancency matrix (which might be sparse) where A_{ij} = 1 if there is an edge and 0 otherwise between two nodes i and j\n",
        "* The feature vectors for all the nodes"
      ]
    },
    {
      "metadata": {
        "id": "KPMzGtTg7_Mp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Synthetic data "
      ]
    },
    {
      "metadata": {
        "id": "S8h-eAoXKYTu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_entry_values(coordinates, latent_vectors):\n",
        "    \"\"\"\n",
        "    coordinates: list of coordinates\n",
        "    \n",
        "    \"\"\"\n",
        "    vals    = list()\n",
        "    for i, j in coordinates:\n",
        "        if np.sum(latent_vectors[i] * latent_vectors[j]) > 0:\n",
        "            vals.append(1)\n",
        "        else:\n",
        "            vals.append(0)\n",
        "    return vals\n",
        "    \n",
        "\n",
        "def create_sparse_matrix_list(observed_coordinates, num_nodes, latent_vectors):\n",
        "    \"\"\"\n",
        "    observed_coordinates: list of (row,col) coordinates\n",
        "    num_nodes : int, number of nodes\n",
        "    latent_vectors : latent vectos whose inner products used to determine adjacency\n",
        "    matrix\n",
        "    \n",
        "    ---\n",
        "    return the sparse representation of the adjacency matrix\n",
        "    \n",
        "    \"\"\"\n",
        "    sparse_adjacency_matrix = [[] for _ in range(num_nodes)]\n",
        "\n",
        "    for entry in observed_coordinates:\n",
        "        idx, idy = entry[0], entry[1]\n",
        "        inner = np.dot(latent_vectors[idx, :], latent_vectors[idy, :])\n",
        "        val = 0\n",
        "        if inner > 0:\n",
        "            val = 1\n",
        "\n",
        "        sparse_adjacency_matrix[idx].append((entry, val))\n",
        "        sparse_adjacency_matrix[idy].append(([idy, idx], val))\n",
        "\n",
        "    return sparse_adjacency_matrix\n",
        "\n",
        "\n",
        "def create_observed_features(latent_vectors, num_nodes, true_dim, observed_dim, noise_feature_num=0):\n",
        "    \"\"\"    \n",
        "    latent_vectors : shape(num_nodes, true_dim), the latent feature vectors of all nodes\n",
        "    num_nodes : int, number of nodes in this network\n",
        "    true_dim  : int, dimension of the latent feature (the code)\n",
        "    observed_dim : int, dimension of the observed feature vector\n",
        "    num_noisy_dim : int, added noisy dimension\n",
        "    \n",
        "    ---\n",
        "    \n",
        "    Create observed features that contain both true and noisy features\n",
        "\n",
        "    1. Create a random transformation matrix A\n",
        "    2. Apply the transformation A X_l where X_l is the latent feature vectors\n",
        "    3. \n",
        "    ---\n",
        "    returns\n",
        "    \n",
        "    augmented_feature_matrix : shape(num_nodes, observed_dim + num_noisy_dim)\n",
        "    the augmented observed feature vector for all the nodes, with relevent\n",
        "    dimensions together with noisy entries.\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    # Create a random transformation, A\n",
        "    transformation_matrix = np.random.randn(observed_dim, true_dim)\n",
        "\n",
        "    # X_true = dot(A, x_true)\n",
        "    transformed_feature = np.dot(transformation_matrix, latent_vectors.T)\n",
        "    transformed_feature = transformed_feature.T\n",
        "    transformed_feature /= transformed_feature.max()  # Scale the feature vectors by dividing by the max value\n",
        "\n",
        "    # If no noisy feature, just return the transformed feature\n",
        "    if noise_feature_num == 0:\n",
        "        return transformed_feature\n",
        "    \n",
        "    # If there are noisy features, generate these from standard normal distribution\n",
        "    # Noise feature = N(0, I)\n",
        "    # Create noise matrix X_noise\n",
        "    noise_feature_matrix = np.random.randn(num_nodes, noise_feature_num)\n",
        "    augmented_dim = observed_dim + noise_feature_num\n",
        "\n",
        "    # Horizontally concatenate X_true :: X_noise\n",
        "    augmented_feature_matrix = np.hstack((transformed_feature, noise_feature_matrix))\n",
        "\n",
        "    # Permute the features column\n",
        "    augmented_feature_matrix = augmented_feature_matrix[:, np.random.permutation(augmented_dim)]\n",
        "    return augmented_feature_matrix\n",
        "\n",
        "\n",
        "def create_synthetic_data(num_nodes, sparsity, true_dim, observed_dim, num_noisy_dim):\n",
        "    \"\"\"\n",
        "    num_nodes : int, number of nodes in this network\n",
        "    sparsity  : float, ratio of all entries, which are observed\n",
        "    true_dim  : int, dimension of the latent feature (the code)\n",
        "    observed_dim : int, dimension of the observed feature vector\n",
        "    num_noisy_dim : int, added noisy dimension\n",
        "    \n",
        "    --- \n",
        "    returns\n",
        "    \n",
        "    (latent_vectors, observed_feature_vectors, sparse_adjancency_matrix)\n",
        "    \n",
        "    latent_vectors : shape(num_nodes, true_dim), all the latent feature vectors\n",
        "    observed_feature_vectors : shape(num_nodes, observed_dim), all the observed feature vectors\n",
        "    sparse_adjancency_matrix : list[list(entry, value)] \n",
        "    \n",
        "    \"\"\"\n",
        "    # Create num_nodes hidden vector randomly\n",
        "    latent_vectors = np.random.multivariate_normal(np.zeros((true_dim,)), \\\n",
        "                                                   np.eye(true_dim), \\\n",
        "                                                   size=(num_nodes,))\n",
        "\n",
        "    coordinates = [[x,y] for x in range(num_nodes) for y in range(x+1, num_nodes)]\n",
        "    total_num_pairs = len(coordinates)\n",
        "    num_observed = int(len(coordinates) * sparsity)\n",
        "\n",
        "    # Pick a number of random coordinates\n",
        "    observed_idx = np.random.choice(total_num_pairs, num_observed, replace=False)\n",
        "    observed_coordinates = np.take(coordinates, observed_idx, axis=0)\n",
        "    \n",
        "    train_size = int(len(observed_coordinates) * 0.8)\n",
        "    train_coordinates = observed_coordinates[:train_size]\n",
        "    test_coordinates = observed_coordinates[train_size+1:]\n",
        "    \n",
        "    # Create sparse matrix representation\n",
        "    train_sparse_adjacency_matrix = create_sparse_matrix_list(train_coordinates, num_nodes, latent_vectors)\n",
        "    test_sparse_adjacency_matrix = create_sparse_matrix_list(test_coordinates, num_nodes, latent_vectors)\n",
        "    \n",
        "    # Create test matrix and train matrix\n",
        "    observed_feature_vectors = create_observed_features(latent_vectors, num_nodes, true_dim, observed_dim,\n",
        "                                                        num_noisy_dim)\n",
        "\n",
        "    return latent_vectors, observed_feature_vectors, train_sparse_adjacency_matrix, test_sparse_adjacency_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OR95gUPiv3Aq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## KEGG data preprocessing\n",
        "\n",
        "TODO:\n",
        "* Get the list of nodes from G.nodes\n",
        "* Get the list of edges from G.edges\n",
        "and then figure out how to do \"negative\" sampling, namely the absence of edges\n"
      ]
    },
    {
      "metadata": {
        "id": "6ITm9rXywJba",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vr8CH4BrKL6v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#4. Do VAE training\n",
        "\n",
        "TODO:\n",
        "* Update the training function, first generate synthetic data, organize into batches\n",
        "* Extension: Compare between round-robin updates (aka update each row by row where the list of observations always share a common row index) vs mini-batch update where the list of observations can contain any random pair of two indices.\n"
      ]
    },
    {
      "metadata": {
        "id": "_xDkRxRa3mI4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4.1 Evaluation utility function"
      ]
    },
    {
      "metadata": {
        "id": "jBpBZKEVQuiX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to do evaluation\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "def classification_accuracy(model, feature_tensor, adjacency_matrix, report=False):\n",
        "    \"\"\"\n",
        "    Feature vectors.shape -> (num_nodes, number of observed features)\n",
        "    adjancency_matrix -> sparse representation of adjancey matrix\n",
        "    where each row -> list of (coordinates, value) associated with the specific\n",
        "    factor\n",
        "\n",
        "    Uses model.encode(feature_vector)\n",
        "    \"\"\"\n",
        "    latent_vectors = model.encode(feature_tensor)\n",
        "    latent_adj_mat = torch.mm(latent_vectors, latent_vectors.transpose(0 , 1))\n",
        "    num_accurate = 0.0\n",
        "    num_observed = 0.0\n",
        "    for row in adjacency_matrix:\n",
        "        for coor, val in row:\n",
        "            if (\n",
        "                latent_adj_mat[coor[0]][coor[1]].item() < 0.0 and val == 0.0 or\n",
        "                latent_adj_mat[coor[0]][coor[1]].item() >= 0.0 and val == 1.0\n",
        "            ):\n",
        "\n",
        "                if report:\n",
        "                    print(\"predict: \", latent_adj_mat[coor[0]][coor[1]].item(), \" val: \", val)\n",
        "\n",
        "                num_accurate += 1\n",
        "            num_observed += 1\n",
        "\n",
        "    return num_accurate/ num_observed\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bt80bWYI3pHq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4.2 Training on synthetic data"
      ]
    },
    {
      "metadata": {
        "id": "EJ4zwglopm5E",
        "colab_type": "code",
        "outputId": "64a3cec8-0db3-44b9-b0be-a674d218f0da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "def train_on_synthetic_data(model, num_nodes, observed_dim, true_dim, fake_dim):\n",
        "    sparsity = 0.25\n",
        "    true_vectors, feature_vectors, train_adjacency_matrix, test_adjacency_matrix\\\n",
        "                = create_synthetic_data(num_nodes, sparsity, true_dim, observed_dim,\n",
        "                                                                            fake_dim)\n",
        "\n",
        "    # NOTE that there exists the 'interchangeability problem'\n",
        "    # XR^TRX = X^TX\n",
        "    # Y = XR^T\n",
        "    model.fit(feature_vectors, train_adjacency_matrix, n_epochs=1001, test_adjacency_matrix=test_adjacency_matrix)\n",
        "    elbo_epochs = model.elbo_epochs()\n",
        "    loss_epochs = model.loss_epochs()\n",
        "    train_losses = model.tr_losses()\n",
        "    test_losses = model.te_losses()\n",
        "    ELBOs = model.elbos()\n",
        "    \n",
        "\n",
        "    plt.plot(epochs, train_losses, '-', color='b', label='Link predict accuracy on train data')\n",
        "    plt.plot(epochs, test_losses, '--', color='r', label='Link predict accuracy on test data')\n",
        "    plt.ylim((0, 1))\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "    plt.plot(all_epochs, ELBOs, '-', color='r', label='ELBO')\n",
        "    plt.ylabel('ELBO value')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.show()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "num_nodes = 100\n",
        "observed_dim = 8\n",
        "true_dim = 3\n",
        "num_fake_dim = 7\n",
        "\n",
        "hidden_layer_sizes = [300]\n",
        "naive_model = VariationalAutoencoder(n_dims_code=5, \\\n",
        "                               n_dims_data=observed_dim+num_fake_dim, \\\n",
        "                               hidden_layer_sizes=hidden_layer_sizes)\n",
        "\n",
        "train_on_synthetic_data(naive_model, num_nodes, observed_dim, true_dim, num_fake_dim)\n",
        "\n",
        "# inference_engine = FactorizedHierarchicalInvGamma\n",
        "# hsbnn_model = VariationalAutoencoderHSP(inference_engine,\n",
        "#                                n_dims_code=5, \\\n",
        "#                                n_dims_data=observed_dim+num_fake_dim, \\\n",
        "#                                hidden_layer_sizes=hidden_layer_sizes)\n",
        "# train_on_synthetic_data(hsbnn_model, num_nodes, observed_dim, true_dim, num_fake_dim)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:  0  - objective loss:  22.1431  - train accuracy:  0.7007  - test accuracy:  0.6437\n",
            "epoch:  1  - objective loss:  23.1091  - train accuracy:  0.7745  - test accuracy:  0.7206\n",
            "epoch:  25  - objective loss:  14.4938  - train accuracy:  0.9414  - test accuracy:  0.8907\n",
            "epoch:  50  - objective loss:  13.9388  - train accuracy:  0.9474  - test accuracy:  0.9069\n",
            "epoch:  100  - objective loss:  12.8947  - train accuracy:  0.9717  - test accuracy:  0.8664\n",
            "epoch:  150  - objective loss:  13.0261  - train accuracy:  0.9757  - test accuracy:  0.8543\n",
            "epoch:  200  - objective loss:  12.3621  - train accuracy:  0.9747  - test accuracy:  0.8704\n",
            "epoch:  250  - objective loss:  12.2689  - train accuracy:  0.9767  - test accuracy:  0.8664\n",
            "epoch:  300  - objective loss:  12.5803  - train accuracy:  0.9848  - test accuracy:  0.8704\n",
            "epoch:  350  - objective loss:  12.7718  - train accuracy:  0.9858  - test accuracy:  0.8785\n",
            "epoch:  400  - objective loss:  12.7602  - train accuracy:  0.9858  - test accuracy:  0.8583\n",
            "epoch:  450  - objective loss:  12.4667  - train accuracy:  0.9899  - test accuracy:  0.8704\n",
            "epoch:  500  - objective loss:  12.3409  - train accuracy:  0.9869  - test accuracy:  0.8745\n",
            "epoch:  550  - objective loss:  12.0875  - train accuracy:  0.9889  - test accuracy:  0.8785\n",
            "epoch:  600  - objective loss:  12.4647  - train accuracy:  0.9909  - test accuracy:  0.8826\n",
            "epoch:  650  - objective loss:  12.8593  - train accuracy:  0.9879  - test accuracy:  0.8785\n",
            "epoch:  700  - objective loss:  12.4321  - train accuracy:  0.9899  - test accuracy:  0.8785\n",
            "epoch:  750  - objective loss:  12.1582  - train accuracy:  0.9909  - test accuracy:  0.8745\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "J8vNU2I4AuR0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 5. Do HS-VAE training"
      ]
    },
    {
      "metadata": {
        "id": "8Ad9F95hAy5E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5.1 Training on synthetic data"
      ]
    },
    {
      "metadata": {
        "id": "xKHBe8sAAx0d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "dbab20f8-25c8-4a53-a779-ed1555b262fb"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 1. Create synthetic data\n",
        "\n",
        "num_nodes = 20\n",
        "observed_dim = 8\n",
        "true_dim = 3\n",
        "num_fake_dim = 1\n",
        "\n",
        "total_dim = observed_dim + num_fake_dim\n",
        "\n",
        "sparsity = 0.25\n",
        "true_vectors, feature_vectors, train_adjacency_matrix, test_adjacency_matrix\\\n",
        "                = create_synthetic_data(num_nodes, sparsity, true_dim, observed_dim, num_fake_dim)\n",
        "\n",
        "\n",
        "# Step 2. Initialize horseshoe VAE\n",
        "\n",
        "hidden_layer_sizes = [32]\n",
        "\n",
        "hs_vae = HS_VAE(q_sigma=0.2,\n",
        "            n_dims_code=3,\n",
        "            n_dims_data=total_dim,\n",
        "            hidden_layer_sizes=hidden_layer_sizes,\n",
        "            classification=True,\n",
        "            batch_size=20,\n",
        "            lambda_b_global=1.0,\n",
        "            warm_up=False,\n",
        "            polyak=False)\n",
        "\n",
        "# Training\n",
        "hs_vae.fit(feature_vectors, train_adjacency_matrix, n_epochs=1000, test_adjacency_matrix=test_adjacency_matrix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0.0 elbo -261.91770697166385 train-accuracy 0.6216216216216216 test-accuracy 0.4444444444444444\n",
            "Epoch 10.0 elbo -184.65708081493426 train-accuracy 1.0 test-accuracy 0.8888888888888888\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}