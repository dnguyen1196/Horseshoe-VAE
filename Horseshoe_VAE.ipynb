{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Horseshoe-VAE.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "-fsaZQ4qqUxL",
        "LiB6j9qcB4L7",
        "lDDvGLPR69XB",
        "WtqVJIMR_-28",
        "1hvsSyof7EQh",
        "IdhSSbsHzrsB",
        "E72JYLr6TbU8",
        "KPMzGtTg7_Mp",
        "OR95gUPiv3Aq",
        "Vr8CH4BrKL6v",
        "_xDkRxRa3mI4",
        "bt80bWYI3pHq"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dnguyen1196/Horseshoe-VAE/blob/master/Horseshoe_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "1tL3zPeo7h8I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 0. Set up"
      ]
    },
    {
      "metadata": {
        "id": "-fsaZQ4qqUxL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download KEGG file from GGdrive\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "D-GE8kpfgHoI",
        "colab_type": "code",
        "outputId": "4340fce6-0e09-4d61-edb5-3f275c9fa0a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1204
        }
      },
      "cell_type": "code",
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-051b0bad8bf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Authenticate and create the PyDrive client.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# This only needs to be done once per notebook.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthenticate_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mgauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGoogleAuth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mgauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredentials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGoogleCredentials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_application_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/auth.py\u001b[0m in \u001b[0;36mauthenticate_user\u001b[0;34m(clear_output)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcontext_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemporary\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mclear_output\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_noop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m       \u001b[0m_gcloud_login\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0m_install_adc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_check_adc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/auth.py\u001b[0m in \u001b[0;36m_gcloud_login\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m# https://github.com/jupyter/notebook/issues/3159\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_getpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n\\nEnter verification code: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0mgcloud_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m         )\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "kmDCJqexrWNu",
        "colab_type": "code",
        "outputId": "9c8731bf-aebd-498b-b584-a39e77e51ed0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        }
      },
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "G = nx.read_gpickle(\"/content/drive/My Drive/BDL_project/kegg.ungraph.pkl\")\n",
        "\n",
        "for n in G.nodes:\n",
        "    print(\"Fingerprints for node %s: %r\" % (n, G.nodes[n][\"fingerprint\"]))\n",
        "    break\n",
        "    \n",
        "for e in G.edges:\n",
        "    print(e)\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-170376023d06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_gpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/BDL_project/kegg.ungraph.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fingerprints for node %s: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fingerprint\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-706>\u001b[0m in \u001b[0;36mread_gpickle\u001b[0;34m(path)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/networkx/utils/decorators.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(func_to_be_decorated, *args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_string_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0mfobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dispatch_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m             \u001b[0mclose_fobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'read'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/BDL_project/kegg.ungraph.pkl'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "LiB6j9qcB4L7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Install torch and autograd\n",
        "\n",
        "NOTE:\n",
        "Compared to the reference VAE implementation, the main difference is in the calc_vi_loss function\n",
        "where the likelihood term has been augmented with the loss term to reconstruct the adjacency matrix\n"
      ]
    },
    {
      "metadata": {
        "id": "jHLhxy06GNQM",
        "colab_type": "code",
        "outputId": "930089cd-f4c8-4ae8-9ad5-591fc4b52296",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "cell_type": "code",
      "source": [
        "# Install the necssary packages\n",
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "!pip install autograd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting autograd\n",
            "  Downloading https://files.pythonhosted.org/packages/08/7a/1ccee2a929d806ba3dbe632a196ad6a3f1423d6e261ae887e5fef2011420/autograd-1.2.tar.gz\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from autograd) (1.14.6)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from autograd) (0.16.0)\n",
            "Building wheels for collected packages: autograd\n",
            "  Running setup.py bdist_wheel for autograd ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/72/6f/c2/40f130cca2c91f31d354bf72de282922479c09ce0b7853c4c5\n",
            "Successfully built autograd\n",
            "Installing collected packages: autograd\n",
            "Successfully installed autograd-1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qOLmcpS_0MJE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1. VAE with horseshoe priors\n",
        "\n",
        "TODO:\n",
        "* Implement a Neural Network using autograd (NOT pytorch) for compatibility with FactorizedHierarchicalInvGamma\n",
        "* Create a class with similar interface as AE and VAE -> calc_vi_loss function"
      ]
    },
    {
      "metadata": {
        "id": "lDDvGLPR69XB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Util functions\n"
      ]
    },
    {
      "metadata": {
        "id": "VlgTvYFF7BLp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import autograd.numpy.random as npr\n",
        "import autograd.numpy as ag_np\n",
        "from autograd.scipy.misc import logsumexp\n",
        "from autograd.scipy.special import gammaln, psi\n",
        "from src.utility_functions import diag_gaussian_entropy, inv_gamma_entropy, log_normal_entropy\n",
        "from autograd import grad\n",
        "from hs_vae.base_autoencoder import VAE\n",
        "import copy\n",
        "import numpy as np\n",
        "from autograd.scipy.special import gammaln, psi\n",
        "\n",
        "\n",
        "def diag_gaussian_entropy(log_std, D):\n",
        "    return 0.5 * D * (1.0 + np.log(2 * np.pi)) + np.sum(log_std)\n",
        "\n",
        "\n",
        "def inv_gamma_entropy(a, b):\n",
        "    return np.sum(a + np.log(b) + gammaln(a) - (1 + a) * psi(a))\n",
        "\n",
        "\n",
        "def log_normal_entropy(log_std, mu, D):\n",
        "    return np.sum(log_std + mu + 0.5) + (D / 2) * np.log(2 * np.pi)\n",
        "\n",
        "\n",
        "def make_batches(n_data, batch_size):\n",
        "    return [slice(i, min(i+batch_size, n_data)) for i in range(0, n_data, batch_size)]\n",
        "\n",
        "\n",
        "def bce_loss(x, x_prime):\n",
        "    temp1 = x * ag_np.log(x_prime + 1e-10)\n",
        "    temp2 = (1 - x) * ag_np.log(1 - x_prime + 1e-10)\n",
        "    bce = -ag_np.sum(temp1 + temp2)\n",
        "    return bce\n",
        "\n",
        "\n",
        "def sigmoid(x, derivative=False):\n",
        "    return x*(1-x) if derivative else 1/(1+ag_np.exp(-x))\n",
        "\n",
        "\n",
        "def adam(grad, x, callback=None, num_iters=100, step_size=0.001, b1=0.9, b2=0.999, eps=10**-8, polyak=False):\n",
        "    \"\"\"Adapted from autograd.misc.optimizers\"\"\"\n",
        "    m = np.zeros(len(x))\n",
        "    v = np.zeros(len(x))\n",
        "    for i in range(num_iters):\n",
        "        g = grad(x, i)\n",
        "        if callback: callback(x, i, g, polyak)\n",
        "        m = (1 - b1) * g      + b1 * m  # First  moment estimate.\n",
        "        v = (1 - b2) * (g**2) + b2 * v  # Second moment estimate.\n",
        "        mhat = m / (1 - b1**(i + 1))    # Bias correction.\n",
        "        vhat = v / (1 - b2**(i + 1))\n",
        "        x = x - step_size*mhat/(np.sqrt(vhat) + eps)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WtqVJIMR_-28",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Neural network and FactorizedInverseGamma inference"
      ]
    },
    {
      "metadata": {
        "id": "yvI_zErO6vIa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\" Uses a non-centered parameterization of the model.\n",
        "    Fully factorized Gaussian + IGamma Variational distribution\n",
        "\tq = N(w_ijl | m_ijl, sigma^2_ijl) N(ln \\tau_kl | params) IGamma(\\lambda_kl| params)\n",
        "\tIGamma(\\tau_l | params) IGamma(\\lambda_l| params)\n",
        "\"\"\"\n",
        "\n",
        "import autograd.numpy.random as npr\n",
        "import autograd.numpy as ag_np\n",
        "from data.utils import *\n",
        "from autograd.scipy.misc import logsumexp\n",
        "from autograd.scipy.special import gammaln, psi\n",
        "from src.utility_functions import diag_gaussian_entropy, inv_gamma_entropy, log_normal_entropy\n",
        "from autograd import grad\n",
        "from hs_vae.base_autoencoder import VAE\n",
        "from src.optimizers import *\n",
        "from copy import copy\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "A neural network class built on autograd for compatibility with Hs prior current implementations\n",
        "\"\"\"\n",
        "class NeuralNetworkAutoGrad():\n",
        "    def __init__(self, nn_structure=[32], n_dims_input=1, n_dims_output=1, \\\n",
        "                 weight_fill_func=np.zeros, bias_fill_func=np.zeros, activation_func=lambda x: ag_np.maximum(0, x)):\n",
        "\n",
        "        self.nn_param_list = []\n",
        "        self.activation_func = activation_func\n",
        "        self.n_dims_input = n_dims_input\n",
        "        self.n_dims_output = n_dims_output\n",
        "\n",
        "        n_hiddens_per_layer_list = [n_dims_input] + nn_structure + [n_dims_output]\n",
        "\n",
        "        # Given full network size list is [a, b, c, d, e]\n",
        "        # For loop should loop over (a,b) , (b,c) , (c,d) , (d,e)\n",
        "        for n_in, n_out in zip(n_hiddens_per_layer_list[:-1], n_hiddens_per_layer_list[1:]):\n",
        "            self.nn_param_list.append(\n",
        "                dict(\n",
        "                    w=weight_fill_func((n_in, n_out)),\n",
        "                    b=bias_fill_func((n_out,)),\n",
        "                ))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer_id, layer_dict in enumerate(self.nn_param_list):\n",
        "            if layer_id == 0:\n",
        "                if x.ndim > 1:\n",
        "                    in_arr = x\n",
        "                else:\n",
        "                    if x.size == self.nn_param_list[0]['w'].shape[0]:\n",
        "                        in_arr = x[ag_np.newaxis, :]\n",
        "                    else:\n",
        "                        in_arr = x[:, ag_np.newaxis]\n",
        "            else:\n",
        "                in_arr = self.activation_func(out_arr)\n",
        "            out_arr = ag_np.dot(in_arr, layer_dict['w']) + layer_dict['b']\n",
        "        return ag_np.squeeze(out_arr)\n",
        "\n",
        "\n",
        "class FactorizedHierarchicalInvGamma:\n",
        "    def __init__(self, n_weights, lambda_a, lambda_b, lambda_b_global, tau_a, shapes, train_stats, classification=True,\n",
        "                 n_data=None):\n",
        "        self.name = \"Factorized Hierarchical Inverse Gamma Variational Approximation\"\n",
        "        self.classification = classification\n",
        "        self.n_weights = n_weights\n",
        "        self.shapes = shapes\n",
        "        self.num_hidden_layers = len(shapes) - 1\n",
        "        self.lambda_a_prior = lambda_a\n",
        "        self.lambda_b_prior = lambda_b\n",
        "        self.lambda_a_prior_global = 0.5\n",
        "        self.lambda_b_prior_global = lambda_b_global\n",
        "        self.lambda_a_prior_oplayer = 0.5\n",
        "        self.lambda_b_prior_oplayer = 1.\n",
        "        self.tau_a_prior = tau_a\n",
        "        self.tau_a_prior_global = 0.5\n",
        "        self.tau_a_prior_oplayer = 0.5\n",
        "        self.l2pi = np.log(2 * np.pi)\n",
        "        self.n_data = n_data\n",
        "        self.noise_entropy = None\n",
        "        if not self.classification:\n",
        "            # gamma(6, 6) prior on precision\n",
        "            self.noise_a = 6.\n",
        "            self.noise_b = 6.\n",
        "            self.train_stats = train_stats\n",
        "\n",
        "    ######### PACK UNPACK PARAMS #################################################\n",
        "    def initialize_variational_params(self, param_scale=1):\n",
        "        # Initialize weights\n",
        "        wlist = list()\n",
        "        for m, n in self.shapes:\n",
        "            wlist.append(npr.randn(m * n) * np.sqrt(2 / m))\n",
        "            wlist.append(np.zeros(n))  # bias\n",
        "        w = np.concatenate(wlist)\n",
        "        log_sigma = param_scale * npr.randn(w.shape[0]) - 10.\n",
        "        # initialize scale parameters\n",
        "        self.tot_outputs = 0\n",
        "        for _, num_hl_outputs in self.shapes:\n",
        "            self.tot_outputs += num_hl_outputs\n",
        "        # No hs priors on the outputs\n",
        "        self.tot_outputs = self.tot_outputs - self.shapes[-1][1]\n",
        "        if not self.classification:\n",
        "            tau_mu, tau_log_sigma, tau_global_mu, tau_global_log_sigma, tau_oplayer_mu, tau_oplayer_log_sigma, log_a, \\\n",
        "                                log_b = self.initialize_scale_from_prior()\n",
        "            init_params = np.concatenate([w.ravel(), log_sigma.ravel(),\n",
        "                                          tau_mu.ravel(), tau_log_sigma.ravel(), tau_global_mu.ravel(),\n",
        "                                          tau_global_log_sigma.ravel(), tau_oplayer_mu, tau_oplayer_log_sigma, log_a,\n",
        "                                          log_b])\n",
        "        else:\n",
        "            tau_mu, tau_log_sigma, tau_global_mu, tau_global_log_sigma, tau_oplayer_mu, tau_oplayer_log_sigma = \\\n",
        "                self.initialize_scale_from_prior()\n",
        "            init_params = np.concatenate([w.ravel(), log_sigma.ravel(),\n",
        "                                          tau_mu.ravel(), tau_log_sigma.ravel(), tau_global_mu.ravel(),\n",
        "                                          tau_global_log_sigma.ravel(), tau_oplayer_mu, tau_oplayer_log_sigma])\n",
        "\n",
        "        return init_params\n",
        "\n",
        "    def initialize_scale_from_prior(self):\n",
        "        # scale parameters (hidden + observed),\n",
        "        self.lambda_a_hat = (self.tau_a_prior + self.lambda_a_prior) * np.ones([self.tot_outputs, 1]).ravel()\n",
        "        self.lambda_b_hat = (1.0 / self.lambda_b_prior ** 2) * np.ones([self.tot_outputs, 1]).ravel()\n",
        "        self.lambda_a_hat_global = (self.tau_a_prior_global + self.lambda_a_prior_global)  \\\n",
        "            * np.ones([self.num_hidden_layers, 1]).ravel()\n",
        "        self.lambda_b_hat_global = (1.0 / self.lambda_b_prior_global ** 2) * np.ones(\n",
        "            [self.num_hidden_layers, 1]).ravel()\n",
        "        # set oplayer lambda param\n",
        "        self.lambda_a_hat_oplayer = np.array(self.tau_a_prior_oplayer + self.lambda_a_prior_oplayer).reshape(-1)\n",
        "        self.lambda_b_hat_oplayer = (1.0 / self.lambda_b_prior_oplayer ** 2) * np.ones([1]).ravel()\n",
        "        # sample from half cauchy and log to initialize the mean of the log normal\n",
        "        sample = np.abs(self.lambda_b_prior * (npr.randn(self.tot_outputs) / npr.randn(self.tot_outputs)))\n",
        "        tau_mu = np.log(sample)\n",
        "        tau_log_sigma = npr.randn(self.tot_outputs) - 10.\n",
        "        # one tau_global for each hidden layer\n",
        "        sample = np.abs(\n",
        "            self.lambda_b_prior_global * (npr.randn(self.num_hidden_layers) / npr.randn(self.num_hidden_layers)))\n",
        "        tau_global_mu = np.log(sample)\n",
        "        tau_global_log_sigma = npr.randn(self.num_hidden_layers) - 10.\n",
        "        # one tau for all op layer weights\n",
        "        sample = np.abs(self.lambda_b_hat_oplayer * (npr.randn() / npr.randn()))\n",
        "        tau_oplayer_mu = np.log(sample)\n",
        "        tau_oplayer_log_sigma = npr.randn(1) - 10.\n",
        "        if not self.classification:\n",
        "            log_a = np.array(np.log(self.noise_a)).reshape(-1)\n",
        "            log_b = np.array(np.log(self.noise_b)).reshape(-1)\n",
        "            return tau_mu, tau_log_sigma, tau_global_mu, tau_global_log_sigma, tau_oplayer_mu, tau_oplayer_log_sigma, \\\n",
        "                   log_a, log_b\n",
        "        else:\n",
        "            return tau_mu, tau_log_sigma, tau_global_mu, tau_global_log_sigma, tau_oplayer_mu, tau_oplayer_log_sigma\n",
        "\n",
        "    def unpack_params(self, params):\n",
        "        # unpack params\n",
        "        w_vect = params[:self.n_weights]\n",
        "        num_std = 2 * self.n_weights\n",
        "        sigma = np.log(1 + np.exp(params[self.n_weights:num_std]))\n",
        "        tau_mu = params[num_std:num_std + self.tot_outputs]\n",
        "        tau_sigma = np.log(\n",
        "            1 + np.exp(params[num_std + self.tot_outputs:num_std + 2 * self.tot_outputs]))\n",
        "        tau_mu_global = params[num_std + 2 * self.tot_outputs: num_std + 2 * self.tot_outputs + self.num_hidden_layers]\n",
        "        tau_sigma_global = np.log(1 + np.exp(params[num_std + 2 * self.tot_outputs + self.num_hidden_layers:num_std +\n",
        "                                                                    2 * self.tot_outputs + 2 * self.num_hidden_layers]))\n",
        "        tau_mu_oplayer = params[num_std + 2 * self.tot_outputs + 2 * self.num_hidden_layers: num_std +\n",
        "                                                                2 * self.tot_outputs + 2 * self.num_hidden_layers + 1]\n",
        "        tau_sigma_oplayer = np.log(\n",
        "            1 + np.exp(params[num_std + 2 * self.tot_outputs + 2 * self.num_hidden_layers + 1:]))\n",
        "        if not self.classification:\n",
        "            a = tau_sigma_oplayer[1]\n",
        "            b = tau_sigma_oplayer[2]\n",
        "            tau_sigma_oplayer = tau_sigma_oplayer[0]\n",
        "            egamma = a / b\n",
        "            elog_gamma = psi(a) - np.log(b)\n",
        "            self.noise_entropy = inv_gamma_entropy(a, b)\n",
        "            #  we will just use a point estimate of noise_var b/a+1 (noise_var ~ IGamma) for computing predictive ll\n",
        "            self.noisevar = (b / (a + 1)) * self.train_stats['sigma'] ** 2\n",
        "            return w_vect, sigma, tau_mu, tau_sigma, tau_mu_global, tau_sigma_global, tau_mu_oplayer, \\\n",
        "                   tau_sigma_oplayer, elog_gamma, egamma\n",
        "        else:\n",
        "            return w_vect, sigma, tau_mu, tau_sigma, tau_mu_global, tau_sigma_global, tau_mu_oplayer, tau_sigma_oplayer\n",
        "\n",
        "    def unpack_layer_weight_variances(self, sigma_vect):\n",
        "        for m, n in self.shapes:\n",
        "            yield sigma_vect[:m * n].reshape((m, n)), sigma_vect[m * n:m * n + n]\n",
        "            sigma_vect = sigma_vect[(m + 1) * n:]\n",
        "\n",
        "    def unpack_layer_weight_priors(self, tau_vect):\n",
        "        for m, n in self.shapes:\n",
        "            yield tau_vect[:n]\n",
        "            tau_vect = tau_vect[n:]\n",
        "\n",
        "    def unpack_layer_weights(self, w_vect):\n",
        "        for m, n in self.shapes:\n",
        "            yield w_vect[:m * n].reshape((m, n)), w_vect[m * n:m * n + n]\n",
        "            w_vect = w_vect[(m + 1) * n:]\n",
        "\n",
        "    ######### Fixed Point Updates ################################## #####\n",
        "    def fixed_point_updates(self, params):\n",
        "        if self.classification:\n",
        "            w_vect, sigma, tau_mu, tau_sigma, tau_mu_global, tau_sigma_global, tau_mu_oplayer, tau_sigma_oplayer = \\\n",
        "                self.unpack_params(params)\n",
        "        else:\n",
        "            w_vect, sigma, tau_mu, tau_sigma, tau_mu_global, tau_sigma_global, tau_mu_oplayer, tau_sigma_oplayer, _, _ \\\n",
        "                = self.unpack_params(params)\n",
        "        # update lambda moments\n",
        "        self.lambda_b_hat = np.exp(-tau_mu + 0.5 * tau_sigma ** 2) + (1. / self.lambda_b_prior ** 2)\n",
        "        self.lambda_b_hat_global = np.exp(-tau_mu_global + 0.5 * tau_sigma_global ** 2) + (\n",
        "            1. / self.lambda_b_prior_global ** 2)\n",
        "        self.lambda_b_hat_oplayer = np.exp(-tau_mu_oplayer + 0.5 * tau_sigma_oplayer ** 2) + (\n",
        "            1. / self.lambda_b_prior_oplayer ** 2)\n",
        "        return None\n",
        "\n",
        "    ######### ELBO CALC ################################################\n",
        "    def forward(self, mu_vect, sigma_vect, tau_mu_vect, tau_sigma_vect, tau_mu_global, tau_sigma_global,\n",
        "                tau_mu_oplayer, tau_sigma_oplayer, inputs):\n",
        "        for layer_id, (mu, var, tau_mu, tau_sigma) in enumerate(\n",
        "                zip(self.unpack_layer_weights(mu_vect), self.unpack_layer_weight_variances(sigma_vect),\n",
        "                    self.unpack_layer_weight_priors(tau_mu_vect),\n",
        "                    self.unpack_layer_weight_priors(tau_sigma_vect))):\n",
        "            w, b = mu\n",
        "            sigma__w, sigma_b = var\n",
        "            if layer_id < len(self.shapes) - 1:\n",
        "                scale_mu = 0.5 * (tau_mu + tau_mu_global[layer_id])\n",
        "                scale_v = 0.25 * (tau_sigma ** 2 + tau_sigma_global[layer_id] ** 2)\n",
        "                scale = np.exp(scale_mu + np.sqrt(scale_v) * npr.randn(tau_mu.shape[0]))\n",
        "                mu_w = np.dot(inputs, w) + b\n",
        "                v_w = np.dot(inputs ** 2, sigma__w ** 2) + sigma_b ** 2\n",
        "                outputs = (np.sqrt(v_w) / np.sqrt(inputs.shape[1])) * np.random.normal(size=mu_w.shape) + mu_w\n",
        "                outputs = scale * outputs\n",
        "                inputs = outputs * (outputs > 0)\n",
        "            else:\n",
        "                op_scale_mu = 0.5 * tau_mu_oplayer\n",
        "                op_scale_v = 0.25 * tau_sigma_oplayer ** 2\n",
        "                Ekappa_half = np.exp(op_scale_mu + np.sqrt(op_scale_v) * npr.randn())\n",
        "                mu_w = np.dot(inputs, w) + b\n",
        "                v_w = np.dot(inputs ** 2, sigma__w ** 2) + sigma_b ** 2\n",
        "                outputs = Ekappa_half * (np.sqrt(v_w) / np.sqrt(inputs.shape[1])) * np.random.normal(\n",
        "                    size=mu_w.shape) + mu_w\n",
        "        return outputs\n",
        "\n",
        "    def EPw_Gaussian(self, prior_precision, w, sigma):\n",
        "        \"\"\"\"\\int q(z) log p(z) dz, assuming gaussian q(z) and p(z)\"\"\"\n",
        "        wD = w.shape[0]\n",
        "        prior_wvar_ = 1. / prior_precision\n",
        "        a = - 0.5 * wD * np.log(2 * np.pi) - 0.5 * wD * np.log(prior_wvar_) - 0.5 * prior_precision * (\n",
        "            np.dot(w.T, w) + np.sum((sigma ** 2)))\n",
        "        return a\n",
        "\n",
        "    def EP_Gamma(self, Egamma, Elog_gamma):\n",
        "        \"\"\" Enoise precision \"\"\"\n",
        "        return self.noise_a * np.log(self.noise_b) - gammaln(self.noise_a) + (\n",
        "                                                            - self.noise_a - 1) * Elog_gamma - self.noise_b * Egamma\n",
        "\n",
        "    def EPtaulambda(self, tau_mu, tau_sigma, tau_a_prior, lambda_a_prior,\n",
        "                    lambda_b_prior, lambda_a_hat, lambda_b_hat):\n",
        "        \"\"\" E[ln p(\\tau | \\lambda)] + E[ln p(\\lambda)]\"\"\"\n",
        "        etau_given_lambda = -gammaln(tau_a_prior) - tau_a_prior * (np.log(lambda_b_hat) - psi(lambda_a_hat)) + (\n",
        "                            -tau_a_prior - 1.) * tau_mu - np.exp(-tau_mu + 0.5 * tau_sigma ** 2) * (lambda_a_hat /\n",
        "                                               lambda_b_hat)\n",
        "        elambda = -gammaln(lambda_a_prior) - 2 * lambda_a_prior * np.log(lambda_b_prior) + (-lambda_a_prior - 1.) * (\n",
        "            np.log(lambda_b_hat) - psi(lambda_a_hat)) - (1. / lambda_b_prior ** 2) * (lambda_a_hat / lambda_b_hat)\n",
        "        return np.sum(etau_given_lambda) + np.sum(elambda)\n",
        "\n",
        "    def entropy(self, sigma, tau_sigma, tau_mu, tau_sigma_global, tau_mu_global, tau_sigma_oplayer, tau_mu_oplayer):\n",
        "        ent_w = diag_gaussian_entropy(np.log(sigma), self.n_weights)\n",
        "        ent_tau = log_normal_entropy(np.log(tau_sigma), tau_mu, self.tot_outputs) + log_normal_entropy(\n",
        "            np.log(tau_sigma_global), tau_mu_global, self.num_hidden_layers) + log_normal_entropy(\n",
        "            np.log(tau_sigma_oplayer), tau_mu_oplayer, 1)\n",
        "        ent_lambda = inv_gamma_entropy(self.lambda_a_hat, self.lambda_b_hat) + inv_gamma_entropy(\n",
        "            self.lambda_a_hat_global, self.lambda_b_hat_global) + inv_gamma_entropy(self.lambda_a_hat_oplayer,\n",
        "                                                                                    self.lambda_b_hat_oplayer)\n",
        "        return ent_w, ent_tau, ent_lambda\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1hvsSyof7EQh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## HS_VAE implementations"
      ]
    },
    {
      "metadata": {
        "id": "1cydvYRB0OqL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class HS_VAE(VAE):\n",
        "    def __init__(\n",
        "            self,\n",
        "            q_sigma=0.2,\n",
        "            n_dims_code=16,\n",
        "            n_dims_data=64,\n",
        "            hidden_layer_sizes=[32],\n",
        "            classification=True,\n",
        "            batch_size=128,\n",
        "            lambda_b_global=1.0,\n",
        "            warm_up=False,\n",
        "            polyak=False,):\n",
        "\n",
        "        super(HS_VAE, self).__init__()\n",
        "        layer_sizes = (\n",
        "            [n_dims_data] + hidden_layer_sizes + [n_dims_code]\n",
        "        )\n",
        "        self.n_dims_code = n_dims_code\n",
        "        self.q_sigma = q_sigma\n",
        "        self.n_dims_data = n_dims_data\n",
        "\n",
        "        self.shapes = list(zip(layer_sizes[:-1], layer_sizes[1:]))\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.lambda_b_global = lambda_b_global\n",
        "        self.N_weights = sum((m + 1) * n for m, n in self.shapes)\n",
        "        self.elbo = list()\n",
        "        self.val_ll = list()\n",
        "        self.val_err = list()\n",
        "        self.train_err = list()\n",
        "        self.variational_params = None\n",
        "        self.init_params = None\n",
        "        self.polyak_params = None\n",
        "        self.polyak = polyak\n",
        "        self.variational_params_store = {}\n",
        "        self.optimal_elbo_params = None\n",
        "        self.warm_up = warm_up  # if True, anneal in KL\n",
        "\n",
        "        self.M = 100 # TODO: the total number of mini-batches (Number of nodes?)\n",
        "\n",
        "        # TODO: in Ghosh's implementation, mu is the mean of all x_train and\n",
        "        # TODO: sigma is the standard deviation of all x_train\n",
        "        train_stats = dict()\n",
        "        train_stats['mu'] = 0\n",
        "        train_stats['sigma'] = 1\n",
        "\n",
        "        # Initialize the encoder and decoder\n",
        "        # TODO: incorporate Factorized Hierarchical Inverse Gamma\n",
        "        self.horseshoe_encoder = FactorizedHierarchicalInvGamma(\n",
        "            lambda_a=0.5, lambda_b=1.0,\n",
        "            lambda_b_global=self.lambda_b_global, tau_a=0.5,\n",
        "            shapes=self.shapes, train_stats=train_stats,\n",
        "            classification=classification,\n",
        "            n_weights=self.N_weights)\n",
        "\n",
        "        self.decoder = NeuralNetworkAutoGrad(nn_structure=hidden_layer_sizes, \\\n",
        "                                             n_dims_input=n_dims_code, n_dims_output=n_dims_data)\n",
        "\n",
        "    def neg_elbo(self, params, epoch, xs_ND, ys_ND, matrix_entries):\n",
        "        \"\"\"\n",
        "\n",
        "        :param params:\n",
        "        :param epoch:\n",
        "        :param xs_ND:\n",
        "        :param ys_ND:\n",
        "        :param matrix_entries:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if self.warm_up:\n",
        "            nt = 200  # linear increments between 0 and 1 up to nt (1 after nt)\n",
        "            temperature = epoch / nt\n",
        "            if temperature > 1:\n",
        "                temperature = 1\n",
        "        else:\n",
        "            temperature = 1\n",
        "\n",
        "        # Unpack the current parameters using function provided by Inv-Gamma\n",
        "        w_vect, sigma, tau_mu, tau_sigma, tau_mu_global, tau_sigma_global, tau_mu_oplayer, tau_sigma_oplayer \\\n",
        "                = self.horseshoe_encoder.unpack_params(params)\n",
        "\n",
        "        # Compute the x_NC codes and y_NC codes (latent embeddings)\n",
        "        xs_NC = self.horseshoe_encoder.forward(w_vect, sigma, tau_mu, tau_sigma,\\\n",
        "                                       tau_mu_global, tau_sigma_global, tau_mu_oplayer, tau_sigma_oplayer, xs_ND)\n",
        "\n",
        "        ys_NC = self.horseshoe_encoder.forward(w_vect, sigma, tau_mu, tau_sigma,\\\n",
        "                                       tau_mu_global, tau_sigma_global, tau_mu_oplayer, tau_sigma_oplayer, ys_ND)\n",
        "\n",
        "\n",
        "        # Compute log likelihood\n",
        "        # TODO: check if there is any scaling factor I need to compute based on implementions in InvGamma\n",
        "        log_lik = self.log_likelihood_compute(xs_NC, ys_NC, xs_ND, ys_ND, matrix_entries)\n",
        "\n",
        "        # Compute the entropies and log_prior\n",
        "        # Note that this is taken from implementations from inv-Gamma\n",
        "        log_prior, ent_w, ent_tau, ent_lambda = self.entropy_compute(params)\n",
        "\n",
        "        log_variational = ent_w + ent_tau + ent_lambda\n",
        "        minibatch_rescaling = 1. / self.M\n",
        "        ELBO = temperature * minibatch_rescaling * (log_variational + log_prior) + log_lik\n",
        "        return -1 * ELBO\n",
        "\n",
        "    def entropy_compute(self, params):\n",
        "        w_vect, sigma, tau_mu, tau_sigma, tau_mu_global, tau_sigma_global, tau_mu_oplayer, tau_sigma_oplayer \\\n",
        "                = self.horseshoe_encoder.unpack_params(params)\n",
        "\n",
        "        log_prior = self.horseshoe_encoder.EPw_Gaussian(1., w_vect, sigma)\n",
        "        log_prior = log_prior + \\\n",
        "                    self.horseshoe_encoder.EPtaulambda(tau_mu, tau_sigma, self.horseshoe_encoder.tau_a_prior, \\\n",
        "                                                       self.horseshoe_encoder.lambda_a_prior, self.horseshoe_encoder.lambda_b_prior,\n",
        "                                                       self.horseshoe_encoder.lambda_a_hat, self.horseshoe_encoder.lambda_b_hat) + \\\n",
        "                    self.horseshoe_encoder.EPtaulambda(tau_mu_global, tau_sigma_global, self.horseshoe_encoder.tau_a_prior_global,\n",
        "                                     self.horseshoe_encoder.lambda_a_prior_global, self.horseshoe_encoder.lambda_b_prior_global, self.horseshoe_encoder.lambda_a_hat_global,\n",
        "                                     self.horseshoe_encoder.lambda_b_hat_global) + \\\n",
        "                    self.horseshoe_encoder.EPtaulambda(tau_mu_oplayer, tau_sigma_oplayer, self.horseshoe_encoder.tau_a_prior_oplayer,\n",
        "                                     self.horseshoe_encoder.lambda_a_prior_oplayer, self.horseshoe_encoder.lambda_b_prior_oplayer,\n",
        "                                     self.horseshoe_encoder.lambda_a_hat_oplayer, self.horseshoe_encoder.lambda_b_hat_oplayer)\n",
        "\n",
        "        # Compute the entropies\n",
        "        ent_w, ent_tau, ent_lambda = self.horseshoe_encoder.entropy(sigma, tau_sigma, tau_mu, tau_sigma_global, tau_mu_global,\n",
        "                                                  tau_sigma_oplayer, tau_mu_oplayer)\n",
        "\n",
        "        return log_prior, ent_w, ent_tau, ent_lambda\n",
        "\n",
        "    def log_likelihood_compute(self, xs_NC, ys_NC, xs_ND, ys_NDs, matrix_entries):\n",
        "        \"\"\"\n",
        "        :param xs_NC:\n",
        "        :param ys_NC:\n",
        "        :param xs_ND:\n",
        "        :param ys_NDs:\n",
        "        :param matrix_entries:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        n_mc_samples = 20\n",
        "        log_lik      = 0.\n",
        "\n",
        "        # Generate samples from N(mx_NC, q_sigma) to compute the following\n",
        "        # E_q[log p(x_ND|mx_NC)]\n",
        "        for ss in range(n_mc_samples):\n",
        "            sample_xz_NC = self.draw_sample_from_q(xs_NC)\n",
        "            sample_xproba_ND = self.decode(sample_xz_NC)\n",
        "\n",
        "            sample_yz_NC = self.draw_sample_from_q(ys_NC)\n",
        "            sample_yproba_ND = self.decode(sample_yz_NC)\n",
        "\n",
        "            # Use MSE to measure reconstruction loss\n",
        "            # Since MSE is equivalent to log gaussian loss\n",
        "            log_ll_x_reconstructed = -0.5 * ag_np.sum((sample_xproba_ND - xs_ND)**2)\n",
        "            log_ll_y_reconstructed = -0.5 * ag_np.sum((sample_yproba_ND - ys_NDs)**2)\n",
        "\n",
        "            # KL divergence from q(mu, sigma) to prior (std normal)\n",
        "            log_lik += 1 / n_mc_samples * (log_ll_x_reconstructed + log_ll_y_reconstructed)\n",
        "\n",
        "        # Compute the loss from adjacency matrix reconstruction\n",
        "        # Get number of entries\n",
        "        num_samples = len(matrix_entries)\n",
        "        f_predict = ag_np.zeros(num_samples)\n",
        "\n",
        "        # Compute\n",
        "        # E_q[log p(A_ij|x_i, x_j)] = E_q[Bern(A_ij|sigmoid(x_i dot x_j))]\n",
        "        for ss in range(n_mc_samples):\n",
        "            # These two should have the same shape\n",
        "            # which is (N*C)\n",
        "            sample_xz_NC = self.draw_sample_from_q(xs_NC)\n",
        "            sample_yz_NC = self.draw_sample_from_q(ys_NC)\n",
        "\n",
        "            # inner_prod.shape = (N,)\n",
        "            inner_prod = ag_np.sum(sample_xz_NC * sample_yz_NC, axis=1)\n",
        "            f_predict += 1 / n_mc_samples * sigmoid(inner_prod)\n",
        "\n",
        "        matrix_reconstruction_loss = bce_loss(matrix_entries, f_predict)\n",
        "        log_lik_matrix_reconstruction = -matrix_reconstruction_loss\n",
        "        log_lik += log_lik_matrix_reconstruction\n",
        "        return log_lik\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder.forward(z)\n",
        "\n",
        "    def draw_sample_from_q(self, xs_NC):\n",
        "        N = xs_NC.shape[0]\n",
        "        # The dimension of the code\n",
        "        C = self.n_dims_code\n",
        "        # Draw standard normal samples \"epsilon\"\n",
        "        # Use the reparameterization trick\n",
        "        eps_NC = np.random.randn(N, C)\n",
        "        z_NC = xs_NC + eps_NC * self.q_sigma\n",
        "        return z_NC\n",
        "\n",
        "    def calc_vi_loss(self, params, t):\n",
        "        idx = t % self.M\n",
        "\n",
        "        # TODO: do mini-batch computation here\n",
        "        x_ND = self.feature_vectors[idx]\n",
        "        observed_entries = self.train_adjacency_matrix[idx]\n",
        "        other_vec_idx = [entry[0][1] for entry in observed_entries]\n",
        "        matrix_entries = np.asarray([entry[1] for entry in observed_entries])\n",
        "\n",
        "        xs_ND = []\n",
        "        ys_ND = []\n",
        "        for idy in other_vec_idx:\n",
        "            xs_ND.append(x_ND)\n",
        "            ys_ND.append(self.feature_vectors[idy, :])\n",
        "\n",
        "        xs_ND = ag_np.asarray(xs_ND)\n",
        "        ys_ND = ag_np.asarray(ys_ND)\n",
        "\n",
        "        return self.neg_elbo(params, t / self.M, xs_ND, ys_ND, matrix_entries)\n",
        "\n",
        "\n",
        "    def fit(self, feature_vectors, train_adjacency_matrix, n_epochs=10, l_rate=0.01, test_adjacency_matrix=None):\n",
        "        self.M = feature_vectors.shape[0]\n",
        "        # TODO: Number of nodes is the number of minibatches in an epoch?\n",
        "        self.feature_vectors = feature_vectors\n",
        "        self.train_adjacency_matrix = train_adjacency_matrix\n",
        "\n",
        "        # TODO: have both test and train sets\n",
        "        # TODO: look into how to do mini-batches\n",
        "        self.test_adjacency_matrix = train_adjacency_matrix\n",
        "\n",
        "        def callback(params, t, g, decay=0.999):\n",
        "            if self.polyak:\n",
        "                # exponential moving average.\n",
        "                self.polyak_params = decay * self.polyak_params + (1 - decay) * params\n",
        "\n",
        "            elbo = -self.calc_vi_loss(params, t)\n",
        "            self.elbo.append(elbo)\n",
        "\n",
        "            if (t % self.M) == 0:\n",
        "                train_err = self.compute_train_accuracy(params)\n",
        "\n",
        "                self.train_err.append(train_err)\n",
        "\n",
        "                if ((t / self.M) % 10) == 0:\n",
        "                    print(\"Epoch {} elbo {} train-accuracy {}\".format(t / self.M, self.elbo[-1],\n",
        "                                                                                         train_err ))\n",
        "\n",
        "            if (t % 250) == 0:\n",
        "                # store optimization progress.\n",
        "                self.variational_params_store[t] = copy(params)\n",
        "            if t > 2:\n",
        "                if self.elbo[-1] > max(self.elbo[:-1]):\n",
        "                    self.optimal_elbo_params = copy(params)\n",
        "\n",
        "            # update inverse gamma distributions\n",
        "            self.horseshoe_encoder.fixed_point_updates(params)\n",
        "\n",
        "        # Variational parameters include tau, lambda, v, Vu etc\n",
        "        init_var_params = self.horseshoe_encoder.initialize_variational_params()\n",
        "        self.init_params = copy(init_var_params)\n",
        "\n",
        "        if self.polyak:\n",
        "            self.polyak_params = copy(init_var_params)\n",
        "\n",
        "        gradient  = grad(self.calc_vi_loss, 0)\n",
        "        num_iters = n_epochs * self.M  # one iteration = one set of param updates\n",
        "\n",
        "        # Run the algorithm using Adam with callback\n",
        "        self.variational_params = adam(gradient, init_var_params,\n",
        "                                       step_size=l_rate, num_iters=num_iters, callback=callback,\n",
        "                                       polyak=self.polyak)\n",
        "\n",
        "    def compute_train_accuracy(self, params):\n",
        "        W_vect, sigma, tau_mu, tau_sigma, tau_mu_global, tau_sigma_global, tau_mu_oplayer, tau_sigma_oplayer = \\\n",
        "            self.horseshoe_encoder.unpack_params(params)\n",
        "\n",
        "        z_NC = self.horseshoe_encoder.forward(W_vect, sigma, tau_mu, tau_sigma, tau_mu_global, tau_sigma_global,\n",
        "                             tau_mu_oplayer, tau_sigma_oplayer, self.feature_vectors)\n",
        "\n",
        "        # z_NC has shape (N,C)\n",
        "\n",
        "        num_accurate = 0.0\n",
        "        num_observed = 0.0\n",
        "\n",
        "        latent_adj_mat = np.dot(z_NC, z_NC.T)\n",
        "\n",
        "        for row in self.train_adjacency_matrix:\n",
        "            for coor, val in row:\n",
        "                if (latent_adj_mat[coor[0]][coor[1]] < 0.0 and val == 0.0 or\n",
        "                                latent_adj_mat[coor[0]][coor[1]] >= 0.0 and val == 1.0):\n",
        "                    num_accurate += 1\n",
        "                num_observed += 1\n",
        "\n",
        "        return num_accurate / num_observed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IdhSSbsHzrsB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2. Standard VAE class\n",
        "TODO\n",
        "* (Not important) AutoEncoder implementation (not Variational) for comparisons"
      ]
    },
    {
      "metadata": {
        "id": "sYNQvpF8AQYP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            n_dims_code=16,\n",
        "            n_dims_data=32,\n",
        "            hidden_layer_sizes=[32],\n",
        "            encoder=True,):\n",
        "        \n",
        "        \"\"\"\n",
        "        q_sigma = 0.2\n",
        "        \"\"\"\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "\n",
        "        self.n_dims_data = n_dims_data\n",
        "        self.n_dims_code = n_dims_code\n",
        "        layer_sizes = (\n",
        "            [n_dims_data] + hidden_layer_sizes + [n_dims_code]\n",
        "        )\n",
        "        self.n_layers = len(layer_sizes) - 1\n",
        "\n",
        "        if not encoder:\n",
        "            layer_sizes = [a for a in reversed(layer_sizes)]\n",
        "\n",
        "        self.activations = list()\n",
        "        self.params = nn.ModuleList()\n",
        "        for (n_in, n_out) in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
        "            self.params.append(nn.Linear(n_in, n_out))\n",
        "            self.activations.append(F.relu)\n",
        "        self.activations[-1] = lambda a: a\n",
        "            \n",
        "    def forward(self, x):\n",
        "        # Note that if x contains multiple instance\n",
        "        # if x.shape = (num_sample, in_dim)\n",
        "        # then the output shape will be (num_sample, out_dim)\n",
        "        cur_arr = x\n",
        "        for ll in range(self.n_layers):\n",
        "            linear_func = self.params[ll]\n",
        "            a_func = self.activations[ll]\n",
        "            cur_arr = a_func(linear_func(cur_arr))\n",
        "        mu_NC = cur_arr\n",
        "        return mu_NC\n",
        "      \n",
        "      \n",
        "class VariationalAutoencoder(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            q_sigma=0.2,\n",
        "            n_dims_code=16,\n",
        "            n_dims_data=64,\n",
        "            hidden_layer_sizes=[32],\n",
        "    ):\n",
        "\n",
        "        super(VariationalAutoencoder, self).__init__()\n",
        "        self.n_dims_data = n_dims_data\n",
        "        self.n_dims_code = n_dims_code\n",
        "        self.q_sigma = torch.Tensor([float(q_sigma)])\n",
        "        \n",
        "        # Encoder network\n",
        "        self.encoder = NeuralNetwork(\n",
        "            n_dims_code=n_dims_code,\n",
        "            n_dims_data=n_dims_data,\n",
        "            hidden_layer_sizes=hidden_layer_sizes,\n",
        "            encoder=True,\n",
        "        )\n",
        "        # Decoder network\n",
        "        self.decoder = NeuralNetwork(\n",
        "            n_dims_code=n_dims_code,\n",
        "            n_dims_data=n_dims_data,\n",
        "            hidden_layer_sizes=hidden_layer_sizes,\n",
        "            encoder=False,\n",
        "        )\n",
        "        self.train_losses = []\n",
        "        self.test_losses = []\n",
        "        self.ELBOs = []\n",
        "        self.epochs = []\n",
        "        self.all_epochs = []\n",
        "        \n",
        "        \n",
        "    def elbo_epochs(self):\n",
        "        return self.all_epochs\n",
        "      \n",
        "    def loss_epochs(self):\n",
        "        return self.epochs\n",
        "        \n",
        "    def elbos(self):\n",
        "        return self.ELBOs\n",
        "        \n",
        "    def tr_losses(self):\n",
        "        return self.train_losses \n",
        "        \n",
        "    def te_losses(self):\n",
        "        return self.test_losses\n",
        "\n",
        "    def forward(self, x_ND):\n",
        "        \"\"\"\n",
        "        Run entire probabilistic autoencoder on input (encode then decode)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        xproba_ND : 1D array, size of x_ND\n",
        "        \"\"\"\n",
        "        mu_NC = self.encode(x_ND)\n",
        "        z_NC = self.draw_sample_from_q(mu_NC)\n",
        "        return self.decode(z_NC), mu_NC\n",
        "\n",
        "    \n",
        "    def draw_sample_from_q(self, mu_NC):\n",
        "        ''' Draw sample from the probabilistic encoder q(z|mu(x), \\sigma)\n",
        "\n",
        "        We assume that \"q\" is Normal with:\n",
        "        * mean mu (argument of this function)\n",
        "        * stddev q_sigma (attribute of this class, use self.q_sigma)\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "        mu_NC : tensor-like, N x C\n",
        "            Mean of the encoding for each of the N images in minibatch.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        z_NC : tensor-like, N x C\n",
        "            Exactly one sample vector for each of the N images in minibatch.\n",
        "        '''\n",
        "        # Number of samples\n",
        "        N = mu_NC.shape[0]\n",
        "        \n",
        "        # The dimension of the code\n",
        "        C = self.n_dims_code\n",
        "\n",
        "        if self.training:\n",
        "            # Draw standard normal samples \"epsilon\"\n",
        "            # Use the reparameterization trick\n",
        "            eps_NC = torch.randn(N, C)\n",
        "            z_NC = mu_NC + eps_NC * self.q_sigma\n",
        "            return z_NC\n",
        "        else:\n",
        "            # For evaluations, we always just use the mean\n",
        "            return mu_NC\n",
        "\n",
        "    def encode(self, x_ND):\n",
        "        \"\"\"\n",
        "        Args\n",
        "        ----\n",
        "        x_ND: the observation vector\n",
        "        \"\"\"\n",
        "        return self.encoder.forward(x_ND)\n",
        "\n",
        "    def decode(self, z_NC):\n",
        "        \"\"\"\n",
        "        Args\n",
        "        ----\n",
        "        z_NC: the code vector\n",
        "        \"\"\"\n",
        "        return self.decoder.forward(z_NC)\n",
        "\n",
        "    def binary_predict_error_rate(self, f_predict, f_true):\n",
        "        \"\"\"\n",
        "        \n",
        "        \"\"\"\n",
        "        length = f_predict.size()[0]\n",
        "        f_predict_binary = (f_predict > 0.5).type(torch.FloatTensor)\n",
        "        error = torch.sum((f_predict_binary - f_true).abs_()) / length\n",
        "        return error\n",
        "\n",
        "    def calc_vi_loss(self, xs_ND, ys_ND, vals, n_mc_samples=1):\n",
        "        \"\"\"\n",
        "        Args\n",
        "\n",
        "        xs_ND: the input feature vectors\n",
        "        ys_ND: the other feature vectors in mini-batch\n",
        "        vals: the entry associated with (x,y)\n",
        "        n_mc_samples: \n",
        "\n",
        "        ----\n",
        "        Returns:\n",
        "        loss\n",
        "\n",
        "        \"\"\"\n",
        "        neg_expected_ll = 0.0\n",
        "        # Given a (potentially) a tensor of observation vectors, \n",
        "        # Encode it into latent space\n",
        "        mx_NC = self.encode(xs_ND)\n",
        "        my_NC = self.encode(ys_ND)\n",
        "        \n",
        "        # Compute the KL divergence\n",
        "        # KL(N(mx_NC, q_sigma) || N(0, I))\n",
        "        kl_xz_NC = -0.5 * torch.sum(1 + torch.log(self.q_sigma ** 2) - mx_NC ** 2 - self.q_sigma ** 2)\n",
        "        kl_yz_NC = -0.5 * torch.sum(1 + torch.log(self.q_sigma ** 2) - my_NC ** 2 - self.q_sigma ** 2)\n",
        "        kl       = kl_xz_NC + kl_yz_NC # Total KL term\n",
        "        \n",
        "        # Generate samples from N(mx_NC, q_sigma) to compute the following\n",
        "        # E_q[log p(x_ND|mx_NC)]\n",
        "        for ss in range(n_mc_samples):\n",
        "            sample_z_NC      = self.draw_sample_from_q(mx_NC)\n",
        "            sample_xproba_ND = self.decode(sample_z_NC)\n",
        "\n",
        "            # Use MSE to measure reconstruction loss\n",
        "            # Since MSE is equivalent to log gaussian loss\n",
        "            sample_mse_loss  = F.mse_loss(sample_xproba_ND, xs_ND)\n",
        "\n",
        "            # KL divergence from q(mu, sigma) to prior (std normal)\n",
        "            neg_expected_ll += 1/n_mc_samples * sample_mse_loss\n",
        "        \n",
        "        \n",
        "        # Compute the loss from adjacency matrix reconstruction\n",
        "        # Get number of entries\n",
        "        num_samples = len(vals)\n",
        "        f_predict   = torch.zeros(num_samples)\n",
        "\n",
        "        # Compute\n",
        "        # E_q[log p(A_ij|x_i, x_j)] = E_q[Bern(A_ij|sigmoid(x_i dot x_j))]\n",
        "        \n",
        "        for ss in range(n_mc_samples):\n",
        "            # These two should have the same shape\n",
        "            # which is (N*C)\n",
        "            sample_z_NC = self.draw_sample_from_q(mx_NC)\n",
        "            sample_y_NC = self.draw_sample_from_q(my_NC)\n",
        "\n",
        "            # inner_prod.shape = (N,)\n",
        "            inner_prod  = torch.sum(sample_z_NC * sample_y_NC, dim=1)\n",
        "            f_predict  += 1/n_mc_samples * torch.sigmoid(inner_prod)\n",
        "\n",
        "        # Use binary cross entry loss, NOTE that this is for\n",
        "        # adjacency matrix whose entry values are 0 and 1\n",
        "        # This will need to change for other types of adjacency matrix value\n",
        "        # Use the binary prediction loss with logits\n",
        "        matrix_reconstruction_loss = \\\n",
        "            F.binary_cross_entropy(f_predict, Variable(torch.FloatTensor(vals)), reduction='sum')\n",
        "        \n",
        "        neg_expected_ll += matrix_reconstruction_loss\n",
        "        \n",
        "        return neg_expected_ll, kl, matrix_reconstruction_loss, sample_xproba_ND\n",
        "      \n",
        "    def fit(self, feature_vectors, train_adjacency_matrix, n_epochs=10, test_adjacency_matrix=None):\n",
        "        # Initialize optimizer\n",
        "        optimizer = torch.optim.SGD(self.parameters(), lr=0.01)\n",
        "\n",
        "        for epoch in range(n_epochs): \n",
        "            # Do round-robin optimization\n",
        "            for idx in range(num_nodes):\n",
        "                x_ND = Variable(torch.FloatTensor([feature_vectors[idx, :]]))\n",
        "                ys_ND = list()\n",
        "                observed_entries = train_adjacency_matrix[idx]\n",
        "                other_vec_idx = [entry[0][1] for entry in observed_entries]\n",
        "                vals = [entry[1] for entry in observed_entries]\n",
        "\n",
        "                for idy in other_vec_idx:\n",
        "                    ys_ND.append(feature_vectors[idy, :])\n",
        "\n",
        "                ys_ND = Variable(torch.FloatTensor(ys_ND))\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # NOTE:\n",
        "                # expected_ll refers to the expected log likelihood term of ELBO\n",
        "                # kl refers to the KL divergence term of the ELBO\n",
        "                # matrix_loss refers to the matrix reconstruction loss\n",
        "                neg_expected_ll, KL, matrix_loss, _ = self.calc_vi_loss(x_ND, ys_ND, vals, n_mc_samples=10)\n",
        "\n",
        "                KL = 1/len(observed_entries) * KL\n",
        "                # TODO: scale the KL term\n",
        "                # ELBO loss = negative expected log likelihood + KL\n",
        "                elbo_loss = neg_expected_ll + KL\n",
        "\n",
        "                elbo_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                self.all_epochs.append(epoch)\n",
        "                self.ELBOs.append(-elbo_loss)\n",
        "\n",
        "\n",
        "            if epoch in [0, 1, 25] or epoch % 50 == 0:\n",
        "                all_vectors = Variable(torch.FloatTensor(feature_vectors))\n",
        "                train_accuracy = classification_accuracy(self, all_vectors, train_adjacency_matrix)\n",
        "                test_accuracy = classification_accuracy(self, all_vectors, test_adjacency_matrix)\n",
        "                self.epochs.append(epoch)\n",
        "                self.train_losses.append(1.0 - train_accuracy)\n",
        "                self.test_losses.append(1.0 - test_accuracy)\n",
        "\n",
        "                print(\"epoch: \", epoch, \" - objective loss: \", np.around(elbo_loss.data.item(),4), \" - train accuracy: \",\n",
        "                      np.around(train_accuracy,4), \" - test accuracy: \", np.around(test_accuracy, 4))\n",
        "\n",
        "      \n",
        "      \n",
        "\n",
        "class VariationalAutoencoderHSP(VAE):\n",
        "    def __init__(\n",
        "            self,\n",
        "            q_sigma=0.2,\n",
        "            n_dims_code=16,\n",
        "            n_dims_data=64,\n",
        "            hidden_layer_sizes=[32],\n",
        "            classification=False,\n",
        "            batch_size=128,\n",
        "            lambda_b_global=1.0,\n",
        "            warm_up=False,\n",
        "            polyak=False,\n",
        "    ):\n",
        "\n",
        "        super(VariationalAutoencoderHSP, self).__init__()\n",
        "        layer_sizes = (\n",
        "            [n_dims_data] + hidden_layer_sizes + [n_dims_code]\n",
        "        )\n",
        "        self.shapes = list(zip(layer_sizes[:-1], layer_sizes[1:]))\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.lambda_b_global = lambda_b_global\n",
        "        self.N_weights = sum((m+1)*n for m, n in self.shapes)\n",
        "        self.elbo = list()\n",
        "        self.val_ll = list()\n",
        "        self.val_err = list()\n",
        "        self.train_err = list()\n",
        "        self.variational_params = None\n",
        "        self.init_params = None\n",
        "        self.polyak_params = None\n",
        "        self.polyak = polyak\n",
        "        self.variational_params_store = {}\n",
        "        self.optimal_elbo_params = None\n",
        "        self.warm_up = warm_up  # if True, anneal in KL\n",
        "        # TODO: FactorizedInv-Gamma\n",
        "        # Use Ghosh's implementation of the factorizedInv-Gamma inference\n",
        "        # engine or reimplement his code with pytorch\n",
        "        self.inference_engine = FactorizedHierarchicalInvGamma(\n",
        "            lambda_a=0.5, lambda_b=1.0,\n",
        "            lambda_b_global=self.lambda_b_global, tau_a=0.5,\n",
        "            shapes=self.shapes, train_stats=train_stats,\n",
        "            classification=classification, n_data=self.X.shape[0],\n",
        "            n_weights=self.N_weights)\n",
        "        \n",
        "    def neg_elbo(self, params, epoch, x, y):\n",
        "        if self.warm_up:\n",
        "            nt = 200  # linear increments between 0 and 1 up to nt (1 after nt)\n",
        "            temperature = epoch/nt\n",
        "            if temperature > 1:\n",
        "                temperature = 1\n",
        "        else:\n",
        "            temperature = 1\n",
        "        log_lik, log_prior, ent_w, ent_tau, ent_lam = self.inference_engine.compute_elbo_contribs(params, x, y)\n",
        "        log_variational = ent_w + ent_tau + ent_lam\n",
        "        minibatch_rescaling = 1./self.M\n",
        "        ELBO = temperature * minibatch_rescaling * (log_variational + log_prior) + log_lik\n",
        "        return -1*ELBO\n",
        "      \n",
        "    def variational_objective(self, params, t):\n",
        "        idx = self.batches[t % self.M]\n",
        "        return self.neg_elbo(params, t/self.M, self.X[idx], self.y[idx])\n",
        "\n",
        "    def compute_optimal_test_ll(self, num_samples=100):\n",
        "        if not self.polyak:\n",
        "            return self.inference_engine.compute_test_ll(self.variational_params, self.X_test,\n",
        "                                                         self.y_test, num_samples=num_samples)\n",
        "        else:\n",
        "            return self.inference_engine.compute_test_ll(self.polyak_params, self.X_test,\n",
        "                                                         self.y_test, num_samples=num_samples)\n",
        "          \n",
        "    def fit(self, X, train_adjacency_matrix, n_epochs=10, l_rate=0.01, test_adjacency_matrix=None):\n",
        "        self.X = X\n",
        "        self.Y = train_adjacency_matrix\n",
        "        self.X_test = X\n",
        "        self.Y_test = test_adjacency_matrix\n",
        "        def callback(params, t, g, decay=0.999):\n",
        "            if self.polyak:\n",
        "                # exponential moving average.\n",
        "                self.polyak_params = decay * self.polyak_params + (1 - decay) * params\n",
        "            score = -self.variational_objective(params, t)\n",
        "            self.elbo.append(score)\n",
        "            if (t % self.M) == 0:\n",
        "                if self.polyak:\n",
        "                    val_ll, val_err = self.inference_engine.compute_test_ll(self.polyak_params, self.X_test, self.y_test)\n",
        "                else:\n",
        "                    val_ll, val_err = self.inference_engine.compute_test_ll(params, self.X_test, self.y_test)\n",
        "                train_err = self.inference_engine.compute_train_err(params, self.X, self.y)\n",
        "                self.val_ll.append(val_ll)\n",
        "                self.val_err.append(val_err)\n",
        "                self.train_err.append(train_err)\n",
        "                if ((t / self.M) % 10) == 0:\n",
        "                    if self.inference_engine.classification:\n",
        "                        print(\"Epoch {} lower bound {} train_err {} test_err {} \".format(t/self.M, self.elbo[-1],\n",
        "                                                                                         train_err,\n",
        "                                                                                         self.val_err[-1]))\n",
        "                    else:\n",
        "                        print(\"Epoch {} lower bound {} train_rmse {} test_rmse {} \".format(t / self.M, self.elbo[-1],\n",
        "                                                                                         train_err, self.val_err[-1]))\n",
        "                # randomly permute batch ordering every epoch\n",
        "                self.batches = np.random.permutation(self.batches)\n",
        "            if (t % 250) == 0:\n",
        "                # store optimization progress.\n",
        "                self.variational_params_store[t] = copy(params)\n",
        "            if t > 2:\n",
        "                if self.elbo[-1] > max(self.elbo[:-1]):\n",
        "                    self.optimal_elbo_params = copy(params)\n",
        "            # update inverse gamma distributions\n",
        "            self.inference_engine.fixed_point_updates(params)\n",
        "\n",
        "        init_var_params = self.inference_engine.initialize_variational_params()\n",
        "        self.init_params = copy(init_var_params)\n",
        "        if self.polyak:\n",
        "            self.polyak_params = copy(init_var_params)\n",
        "        gradient = grad(self.variational_objective, 0)\n",
        "        num_iters = n_epochs * model.M  # one iteration = one set of param updates\n",
        "        self.variational_params = adam(gradient, init_var_params,\n",
        "                                       step_size=l_rate, num_iters=num_iters, callback=callback,\n",
        "                                       polyak=model.polyak)\n",
        "\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E72JYLr6TbU8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3. Data processing\n",
        "\n",
        "\n",
        "Synthetic data (and real life data to test on) should include\n",
        "* Adjancency matrix (which might be sparse) where A_{ij} = 1 if there is an edge and 0 otherwise between two nodes i and j\n",
        "* The feature vectors for all the nodes"
      ]
    },
    {
      "metadata": {
        "id": "KPMzGtTg7_Mp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Synthetic data "
      ]
    },
    {
      "metadata": {
        "id": "S8h-eAoXKYTu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_entry_values(coordinates, latent_vectors):\n",
        "    \"\"\"\n",
        "    coordinates: list of coordinates\n",
        "    \n",
        "    \"\"\"\n",
        "    vals    = list()\n",
        "    for i, j in coordinates:\n",
        "        if np.sum(latent_vectors[i] * latent_vectors[j]) > 0:\n",
        "            vals.append(1)\n",
        "        else:\n",
        "            vals.append(0)\n",
        "    return vals\n",
        "    \n",
        "\n",
        "def create_sparse_matrix_list(observed_coordinates, num_nodes, latent_vectors):\n",
        "    \"\"\"\n",
        "    observed_coordinates: list of (row,col) coordinates\n",
        "    num_nodes : int, number of nodes\n",
        "    latent_vectors : latent vectos whose inner products used to determine adjacency\n",
        "    matrix\n",
        "    \n",
        "    ---\n",
        "    return the sparse representation of the adjacency matrix\n",
        "    \n",
        "    \"\"\"\n",
        "    sparse_adjacency_matrix = [[] for _ in range(num_nodes)]\n",
        "\n",
        "    for entry in observed_coordinates:\n",
        "        idx, idy = entry[0], entry[1]\n",
        "        inner = np.dot(latent_vectors[idx, :], latent_vectors[idy, :])\n",
        "        val = 0\n",
        "        if inner > 0:\n",
        "            val = 1\n",
        "\n",
        "        sparse_adjacency_matrix[idx].append((entry, val))\n",
        "        sparse_adjacency_matrix[idy].append(([idy, idx], val))\n",
        "\n",
        "    return sparse_adjacency_matrix\n",
        "\n",
        "\n",
        "def create_observed_features(latent_vectors, num_nodes, true_dim, observed_dim, noise_feature_num=0):\n",
        "    \"\"\"    \n",
        "    latent_vectors : shape(num_nodes, true_dim), the latent feature vectors of all nodes\n",
        "    num_nodes : int, number of nodes in this network\n",
        "    true_dim  : int, dimension of the latent feature (the code)\n",
        "    observed_dim : int, dimension of the observed feature vector\n",
        "    num_noisy_dim : int, added noisy dimension\n",
        "    \n",
        "    ---\n",
        "    \n",
        "    Create observed features that contain both true and noisy features\n",
        "\n",
        "    1. Create a random transformation matrix A\n",
        "    2. Apply the transformation A X_l where X_l is the latent feature vectors\n",
        "    3. \n",
        "    ---\n",
        "    returns\n",
        "    \n",
        "    augmented_feature_matrix : shape(num_nodes, observed_dim + num_noisy_dim)\n",
        "    the augmented observed feature vector for all the nodes, with relevent\n",
        "    dimensions together with noisy entries.\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    # Create a random transformation, A\n",
        "    transformation_matrix = np.random.randn(observed_dim, true_dim)\n",
        "\n",
        "    # X_true = dot(A, x_true)\n",
        "    transformed_feature = np.dot(transformation_matrix, latent_vectors.T)\n",
        "    transformed_feature = transformed_feature.T\n",
        "    transformed_feature /= transformed_feature.max()  # Scale the feature vectors by dividing by the max value\n",
        "\n",
        "    # If no noisy feature, just return the transformed feature\n",
        "    if noise_feature_num == 0:\n",
        "        return transformed_feature\n",
        "    \n",
        "    # If there are noisy features, generate these from standard normal distribution\n",
        "    # Noise feature = N(0, I)\n",
        "    # Create noise matrix X_noise\n",
        "    noise_feature_matrix = np.random.randn(num_nodes, noise_feature_num)\n",
        "    augmented_dim = observed_dim + noise_feature_num\n",
        "\n",
        "    # Horizontally concatenate X_true :: X_noise\n",
        "    augmented_feature_matrix = np.hstack((transformed_feature, noise_feature_matrix))\n",
        "\n",
        "    # Permute the features column\n",
        "    augmented_feature_matrix = augmented_feature_matrix[:, np.random.permutation(augmented_dim)]\n",
        "    return augmented_feature_matrix\n",
        "\n",
        "\n",
        "def create_synthetic_data(num_nodes, sparsity, true_dim, observed_dim, num_noisy_dim):\n",
        "    \"\"\"\n",
        "    num_nodes : int, number of nodes in this network\n",
        "    sparsity  : float, ratio of all entries, which are observed\n",
        "    true_dim  : int, dimension of the latent feature (the code)\n",
        "    observed_dim : int, dimension of the observed feature vector\n",
        "    num_noisy_dim : int, added noisy dimension\n",
        "    \n",
        "    --- \n",
        "    returns\n",
        "    \n",
        "    (latent_vectors, observed_feature_vectors, sparse_adjancency_matrix)\n",
        "    \n",
        "    latent_vectors : shape(num_nodes, true_dim), all the latent feature vectors\n",
        "    observed_feature_vectors : shape(num_nodes, observed_dim), all the observed feature vectors\n",
        "    sparse_adjancency_matrix : list[list(entry, value)] \n",
        "    \n",
        "    \"\"\"\n",
        "    # Create num_nodes hidden vector randomly\n",
        "    latent_vectors = np.random.multivariate_normal(np.zeros((true_dim,)), \\\n",
        "                                                   np.eye(true_dim), \\\n",
        "                                                   size=(num_nodes,))\n",
        "\n",
        "    coordinates = [[x,y] for x in range(num_nodes) for y in range(x+1, num_nodes)]\n",
        "    total_num_pairs = len(coordinates)\n",
        "    num_observed = int(len(coordinates) * sparsity)\n",
        "\n",
        "    # Pick a number of random coordinates\n",
        "    observed_idx = np.random.choice(total_num_pairs, num_observed, replace=False)\n",
        "    observed_coordinates = np.take(coordinates, observed_idx, axis=0)\n",
        "    \n",
        "    train_size = int(len(observed_coordinates) * 0.8)\n",
        "    train_coordinates = observed_coordinates[:train_size]\n",
        "    test_coordinates = observed_coordinates[train_size+1:]\n",
        "    \n",
        "    # Create sparse matrix representation\n",
        "    train_sparse_adjacency_matrix = create_sparse_matrix_list(train_coordinates, num_nodes, latent_vectors)\n",
        "    test_sparse_adjacency_matrix = create_sparse_matrix_list(test_coordinates, num_nodes, latent_vectors)\n",
        "    \n",
        "    # Create test matrix and train matrix\n",
        "    observed_feature_vectors = create_observed_features(latent_vectors, num_nodes, true_dim, observed_dim,\n",
        "                                                        num_noisy_dim)\n",
        "\n",
        "    return latent_vectors, observed_feature_vectors, train_sparse_adjacency_matrix, test_sparse_adjacency_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OR95gUPiv3Aq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## KEGG data preprocessing\n",
        "\n",
        "TODO:\n",
        "* Get the list of nodes from G.nodes\n",
        "* Get the list of edges from G.edges\n",
        "and then figure out how to do \"negative\" sampling, namely the absence of edges\n"
      ]
    },
    {
      "metadata": {
        "id": "6ITm9rXywJba",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vr8CH4BrKL6v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#4. Do VAE training\n",
        "\n",
        "TODO:\n",
        "* Update the training function, first generate synthetic data, organize into batches\n",
        "* Extension: Compare between round-robin updates (aka update each row by row where the list of observations always share a common row index) vs mini-batch update where the list of observations can contain any random pair of two indices.\n"
      ]
    },
    {
      "metadata": {
        "id": "_xDkRxRa3mI4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4.1 Evaluation utility function"
      ]
    },
    {
      "metadata": {
        "id": "jBpBZKEVQuiX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to do evaluation\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "def classification_accuracy(model, feature_tensor, adjacency_matrix, report=False):\n",
        "    \"\"\"\n",
        "    Feature vectors.shape -> (num_nodes, number of observed features)\n",
        "    adjancency_matrix -> sparse representation of adjancey matrix\n",
        "    where each row -> list of (coordinates, value) associated with the specific\n",
        "    factor\n",
        "\n",
        "    Uses model.encode(feature_vector)\n",
        "    \"\"\"\n",
        "    latent_vectors = model.encode(feature_tensor)\n",
        "    latent_adj_mat = torch.mm(latent_vectors, latent_vectors.transpose(0 , 1))\n",
        "    num_accurate = 0.0\n",
        "    num_observed = 0.0\n",
        "    for row in adjacency_matrix:\n",
        "        for coor, val in row:\n",
        "            if (\n",
        "                latent_adj_mat[coor[0]][coor[1]].item() < 0.0 and val == 0.0 or\n",
        "                latent_adj_mat[coor[0]][coor[1]].item() >= 0.0 and val == 1.0\n",
        "            ):\n",
        "\n",
        "                if report:\n",
        "                    print(\"predict: \", latent_adj_mat[coor[0]][coor[1]].item(), \" val: \", val)\n",
        "\n",
        "                num_accurate += 1\n",
        "            num_observed += 1\n",
        "\n",
        "    return num_accurate/ num_observed\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bt80bWYI3pHq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4.2 Training on synthetic data"
      ]
    },
    {
      "metadata": {
        "id": "EJ4zwglopm5E",
        "colab_type": "code",
        "outputId": "64a3cec8-0db3-44b9-b0be-a674d218f0da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "def train_on_synthetic_data(model, num_nodes, observed_dim, true_dim, fake_dim):\n",
        "    sparsity = 0.25\n",
        "    true_vectors, feature_vectors, train_adjacency_matrix, test_adjacency_matrix\\\n",
        "                = create_synthetic_data(num_nodes, sparsity, true_dim, observed_dim,\n",
        "                                                                            fake_dim)\n",
        "\n",
        "    # NOTE that there exists the 'interchangeability problem'\n",
        "    # XR^TRX = X^TX\n",
        "    # Y = XR^T\n",
        "    model.fit(feature_vectors, train_adjacency_matrix, n_epochs=1001, test_adjacency_matrix=test_adjacency_matrix)\n",
        "    elbo_epochs = model.elbo_epochs()\n",
        "    loss_epochs = model.loss_epochs()\n",
        "    train_losses = model.tr_losses()\n",
        "    test_losses = model.te_losses()\n",
        "    ELBOs = model.elbos()\n",
        "    \n",
        "\n",
        "    plt.plot(epochs, train_losses, '-', color='b', label='Link predict accuracy on train data')\n",
        "    plt.plot(epochs, test_losses, '--', color='r', label='Link predict accuracy on test data')\n",
        "    plt.ylim((0, 1))\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "    plt.plot(all_epochs, ELBOs, '-', color='r', label='ELBO')\n",
        "    plt.ylabel('ELBO value')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.show()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "num_nodes = 100\n",
        "observed_dim = 8\n",
        "true_dim = 3\n",
        "num_fake_dim = 7\n",
        "\n",
        "hidden_layer_sizes = [300]\n",
        "naive_model = VariationalAutoencoder(n_dims_code=5, \\\n",
        "                               n_dims_data=observed_dim+num_fake_dim, \\\n",
        "                               hidden_layer_sizes=hidden_layer_sizes)\n",
        "\n",
        "train_on_synthetic_data(naive_model, num_nodes, observed_dim, true_dim, num_fake_dim)\n",
        "\n",
        "# inference_engine = FactorizedHierarchicalInvGamma\n",
        "# hsbnn_model = VariationalAutoencoderHSP(inference_engine,\n",
        "#                                n_dims_code=5, \\\n",
        "#                                n_dims_data=observed_dim+num_fake_dim, \\\n",
        "#                                hidden_layer_sizes=hidden_layer_sizes)\n",
        "# train_on_synthetic_data(hsbnn_model, num_nodes, observed_dim, true_dim, num_fake_dim)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:  0  - objective loss:  22.1431  - train accuracy:  0.7007  - test accuracy:  0.6437\n",
            "epoch:  1  - objective loss:  23.1091  - train accuracy:  0.7745  - test accuracy:  0.7206\n",
            "epoch:  25  - objective loss:  14.4938  - train accuracy:  0.9414  - test accuracy:  0.8907\n",
            "epoch:  50  - objective loss:  13.9388  - train accuracy:  0.9474  - test accuracy:  0.9069\n",
            "epoch:  100  - objective loss:  12.8947  - train accuracy:  0.9717  - test accuracy:  0.8664\n",
            "epoch:  150  - objective loss:  13.0261  - train accuracy:  0.9757  - test accuracy:  0.8543\n",
            "epoch:  200  - objective loss:  12.3621  - train accuracy:  0.9747  - test accuracy:  0.8704\n",
            "epoch:  250  - objective loss:  12.2689  - train accuracy:  0.9767  - test accuracy:  0.8664\n",
            "epoch:  300  - objective loss:  12.5803  - train accuracy:  0.9848  - test accuracy:  0.8704\n",
            "epoch:  350  - objective loss:  12.7718  - train accuracy:  0.9858  - test accuracy:  0.8785\n",
            "epoch:  400  - objective loss:  12.7602  - train accuracy:  0.9858  - test accuracy:  0.8583\n",
            "epoch:  450  - objective loss:  12.4667  - train accuracy:  0.9899  - test accuracy:  0.8704\n",
            "epoch:  500  - objective loss:  12.3409  - train accuracy:  0.9869  - test accuracy:  0.8745\n",
            "epoch:  550  - objective loss:  12.0875  - train accuracy:  0.9889  - test accuracy:  0.8785\n",
            "epoch:  600  - objective loss:  12.4647  - train accuracy:  0.9909  - test accuracy:  0.8826\n",
            "epoch:  650  - objective loss:  12.8593  - train accuracy:  0.9879  - test accuracy:  0.8785\n",
            "epoch:  700  - objective loss:  12.4321  - train accuracy:  0.9899  - test accuracy:  0.8785\n",
            "epoch:  750  - objective loss:  12.1582  - train accuracy:  0.9909  - test accuracy:  0.8745\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "J8vNU2I4AuR0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 5. Do HS-VAE training"
      ]
    },
    {
      "metadata": {
        "id": "8Ad9F95hAy5E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5.1 Training on synthetic data"
      ]
    },
    {
      "metadata": {
        "id": "xKHBe8sAAx0d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}